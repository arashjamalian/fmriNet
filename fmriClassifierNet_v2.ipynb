{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50, VGG19\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "#from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import nibabel as nib\n",
    "import re\n",
    "from collections import Counter\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#tf.set_random_seed(SEED)\n",
    "tf.random.set_seed\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cs: CSI1 sess: 1 run: 1\n",
      "cs: CSI1 sess: 1 run: 2\n",
      "cs: CSI1 sess: 1 run: 3\n",
      "cs: CSI1 sess: 1 run: 4\n",
      "cs: CSI1 sess: 1 run: 5\n",
      "cs: CSI1 sess: 1 run: 6\n",
      "cs: CSI1 sess: 1 run: 7\n",
      "cs: CSI1 sess: 1 run: 8\n",
      "cs: CSI1 sess: 1 run: 9\n",
      "cs: CSI1 sess: 1 run: 10\n",
      "cs: CSI1 sess: 2 run: 1\n",
      "cs: CSI1 sess: 2 run: 2\n",
      "cs: CSI1 sess: 2 run: 3\n",
      "cs: CSI1 sess: 2 run: 4\n",
      "cs: CSI1 sess: 2 run: 5\n",
      "cs: CSI1 sess: 2 run: 6\n",
      "cs: CSI1 sess: 2 run: 7\n",
      "cs: CSI1 sess: 2 run: 8\n",
      "cs: CSI1 sess: 2 run: 9\n",
      "cs: CSI1 sess: 2 run: 10\n",
      "cs: CSI1 sess: 3 run: 1\n",
      "cs: CSI1 sess: 3 run: 2\n",
      "cs: CSI1 sess: 3 run: 3\n",
      "cs: CSI1 sess: 3 run: 4\n",
      "cs: CSI1 sess: 3 run: 5\n",
      "cs: CSI1 sess: 3 run: 6\n",
      "cs: CSI1 sess: 3 run: 7\n",
      "cs: CSI1 sess: 3 run: 8\n",
      "cs: CSI1 sess: 3 run: 9\n",
      "cs: CSI1 sess: 3 run: 10\n",
      "cs: CSI1 sess: 4 run: 1\n",
      "cs: CSI1 sess: 4 run: 2\n",
      "cs: CSI1 sess: 4 run: 3\n",
      "cs: CSI1 sess: 4 run: 4\n",
      "cs: CSI1 sess: 4 run: 5\n",
      "cs: CSI1 sess: 4 run: 6\n",
      "cs: CSI1 sess: 4 run: 7\n",
      "cs: CSI1 sess: 4 run: 8\n",
      "cs: CSI1 sess: 4 run: 9\n",
      "cs: CSI1 sess: 5 run: 1\n",
      "cs: CSI1 sess: 5 run: 2\n",
      "cs: CSI1 sess: 5 run: 3\n",
      "cs: CSI1 sess: 5 run: 4\n",
      "cs: CSI1 sess: 5 run: 5\n",
      "cs: CSI1 sess: 5 run: 6\n",
      "cs: CSI1 sess: 5 run: 7\n",
      "cs: CSI1 sess: 5 run: 8\n",
      "cs: CSI1 sess: 5 run: 9\n",
      "cs: CSI1 sess: 5 run: 10\n",
      "cs: CSI1 sess: 6 run: 1\n",
      "cs: CSI1 sess: 6 run: 2\n",
      "cs: CSI1 sess: 6 run: 3\n",
      "cs: CSI1 sess: 6 run: 4\n",
      "cs: CSI1 sess: 6 run: 5\n",
      "cs: CSI1 sess: 6 run: 6\n",
      "cs: CSI1 sess: 6 run: 7\n",
      "cs: CSI1 sess: 6 run: 8\n",
      "cs: CSI1 sess: 6 run: 9\n",
      "cs: CSI1 sess: 7 run: 1\n",
      "cs: CSI1 sess: 7 run: 2\n",
      "cs: CSI1 sess: 7 run: 3\n",
      "cs: CSI1 sess: 7 run: 4\n",
      "cs: CSI1 sess: 7 run: 5\n",
      "cs: CSI1 sess: 7 run: 6\n",
      "cs: CSI1 sess: 7 run: 7\n",
      "cs: CSI1 sess: 7 run: 8\n",
      "cs: CSI1 sess: 7 run: 9\n",
      "cs: CSI1 sess: 7 run: 10\n",
      "cs: CSI1 sess: 8 run: 1\n",
      "cs: CSI1 sess: 8 run: 2\n",
      "cs: CSI1 sess: 8 run: 3\n",
      "cs: CSI1 sess: 8 run: 4\n",
      "cs: CSI1 sess: 8 run: 5\n",
      "cs: CSI1 sess: 8 run: 6\n",
      "cs: CSI1 sess: 8 run: 7\n",
      "cs: CSI1 sess: 8 run: 8\n",
      "cs: CSI1 sess: 8 run: 9\n",
      "cs: CSI1 sess: 9 run: 1\n",
      "cs: CSI1 sess: 9 run: 2\n",
      "cs: CSI1 sess: 9 run: 3\n",
      "cs: CSI1 sess: 9 run: 4\n",
      "cs: CSI1 sess: 9 run: 5\n",
      "cs: CSI1 sess: 9 run: 6\n",
      "cs: CSI1 sess: 9 run: 7\n",
      "cs: CSI1 sess: 9 run: 8\n",
      "cs: CSI1 sess: 9 run: 9\n",
      "cs: CSI1 sess: 10 run: 1\n",
      "cs: CSI1 sess: 10 run: 2\n",
      "cs: CSI1 sess: 10 run: 3\n",
      "cs: CSI1 sess: 10 run: 4\n",
      "cs: CSI1 sess: 10 run: 5\n",
      "cs: CSI1 sess: 10 run: 6\n",
      "cs: CSI1 sess: 10 run: 7\n",
      "cs: CSI1 sess: 10 run: 8\n",
      "cs: CSI1 sess: 10 run: 9\n",
      "cs: CSI1 sess: 10 run: 10\n",
      "cs: CSI1 sess: 11 run: 1\n",
      "cs: CSI1 sess: 11 run: 2\n",
      "cs: CSI1 sess: 11 run: 3\n",
      "cs: CSI1 sess: 11 run: 4\n",
      "cs: CSI1 sess: 11 run: 5\n",
      "cs: CSI1 sess: 11 run: 6\n",
      "cs: CSI1 sess: 11 run: 7\n",
      "cs: CSI1 sess: 11 run: 8\n",
      "cs: CSI1 sess: 11 run: 9\n",
      "cs: CSI1 sess: 12 run: 1\n",
      "cs: CSI1 sess: 12 run: 2\n",
      "cs: CSI1 sess: 12 run: 3\n",
      "cs: CSI1 sess: 12 run: 4\n",
      "cs: CSI1 sess: 12 run: 5\n",
      "cs: CSI1 sess: 12 run: 6\n",
      "cs: CSI1 sess: 12 run: 7\n",
      "cs: CSI1 sess: 12 run: 8\n",
      "cs: CSI1 sess: 12 run: 9\n",
      "cs: CSI1 sess: 13 run: 1\n",
      "cs: CSI1 sess: 13 run: 2\n",
      "cs: CSI1 sess: 13 run: 3\n",
      "cs: CSI1 sess: 13 run: 4\n",
      "cs: CSI1 sess: 13 run: 5\n",
      "cs: CSI1 sess: 13 run: 6\n",
      "cs: CSI1 sess: 13 run: 7\n",
      "cs: CSI1 sess: 13 run: 8\n",
      "cs: CSI1 sess: 13 run: 9\n",
      "cs: CSI1 sess: 14 run: 1\n",
      "cs: CSI1 sess: 14 run: 2\n",
      "cs: CSI1 sess: 14 run: 3\n",
      "cs: CSI1 sess: 14 run: 4\n",
      "cs: CSI1 sess: 14 run: 5\n",
      "cs: CSI1 sess: 14 run: 6\n",
      "cs: CSI1 sess: 14 run: 7\n",
      "cs: CSI1 sess: 14 run: 8\n",
      "cs: CSI1 sess: 14 run: 9\n",
      "cs: CSI1 sess: 15 run: 1\n",
      "cs: CSI1 sess: 15 run: 2\n",
      "cs: CSI1 sess: 15 run: 3\n",
      "cs: CSI1 sess: 15 run: 4\n",
      "cs: CSI1 sess: 15 run: 5\n",
      "cs: CSI1 sess: 15 run: 6\n",
      "cs: CSI1 sess: 15 run: 7\n",
      "cs: CSI1 sess: 15 run: 8\n",
      "cs: CSI1 sess: 15 run: 9\n",
      "cs: CSI1 sess: 15 run: 10\n",
      "\n",
      "cs: CSI2 sess: 1 run: 1\n",
      "cs: CSI2 sess: 1 run: 2\n",
      "cs: CSI2 sess: 1 run: 3\n",
      "cs: CSI2 sess: 1 run: 4\n",
      "cs: CSI2 sess: 1 run: 5\n",
      "cs: CSI2 sess: 1 run: 6\n",
      "cs: CSI2 sess: 1 run: 7\n",
      "cs: CSI2 sess: 1 run: 8\n",
      "cs: CSI2 sess: 1 run: 9\n",
      "cs: CSI2 sess: 1 run: 10\n",
      "cs: CSI2 sess: 2 run: 1\n",
      "cs: CSI2 sess: 2 run: 2\n",
      "cs: CSI2 sess: 2 run: 3\n",
      "cs: CSI2 sess: 2 run: 4\n",
      "cs: CSI2 sess: 2 run: 5\n",
      "cs: CSI2 sess: 2 run: 6\n",
      "cs: CSI2 sess: 2 run: 7\n",
      "cs: CSI2 sess: 2 run: 8\n",
      "cs: CSI2 sess: 2 run: 9\n",
      "cs: CSI2 sess: 3 run: 1\n",
      "cs: CSI2 sess: 3 run: 2\n",
      "cs: CSI2 sess: 3 run: 3\n",
      "cs: CSI2 sess: 3 run: 4\n",
      "cs: CSI2 sess: 3 run: 5\n",
      "cs: CSI2 sess: 3 run: 6\n",
      "cs: CSI2 sess: 3 run: 7\n",
      "cs: CSI2 sess: 3 run: 8\n",
      "cs: CSI2 sess: 3 run: 9\n",
      "cs: CSI2 sess: 3 run: 10\n",
      "cs: CSI2 sess: 4 run: 1\n",
      "cs: CSI2 sess: 4 run: 2\n",
      "cs: CSI2 sess: 4 run: 3\n",
      "cs: CSI2 sess: 4 run: 4\n",
      "cs: CSI2 sess: 4 run: 5\n",
      "cs: CSI2 sess: 4 run: 6\n",
      "cs: CSI2 sess: 4 run: 7\n",
      "cs: CSI2 sess: 4 run: 8\n",
      "cs: CSI2 sess: 4 run: 9\n",
      "cs: CSI2 sess: 4 run: 10\n",
      "cs: CSI2 sess: 5 run: 1\n",
      "cs: CSI2 sess: 5 run: 2\n",
      "cs: CSI2 sess: 5 run: 3\n",
      "cs: CSI2 sess: 5 run: 4\n",
      "cs: CSI2 sess: 5 run: 5\n",
      "cs: CSI2 sess: 5 run: 6\n",
      "cs: CSI2 sess: 5 run: 7\n",
      "cs: CSI2 sess: 5 run: 8\n",
      "cs: CSI2 sess: 5 run: 9\n",
      "cs: CSI2 sess: 5 run: 10\n",
      "cs: CSI2 sess: 6 run: 1\n",
      "cs: CSI2 sess: 6 run: 2\n",
      "cs: CSI2 sess: 6 run: 3\n",
      "cs: CSI2 sess: 6 run: 4\n",
      "cs: CSI2 sess: 6 run: 5\n",
      "cs: CSI2 sess: 6 run: 6\n",
      "cs: CSI2 sess: 6 run: 7\n",
      "cs: CSI2 sess: 6 run: 8\n",
      "cs: CSI2 sess: 6 run: 9\n",
      "cs: CSI2 sess: 7 run: 1\n",
      "cs: CSI2 sess: 7 run: 2\n",
      "cs: CSI2 sess: 7 run: 3\n",
      "cs: CSI2 sess: 7 run: 4\n",
      "cs: CSI2 sess: 7 run: 5\n",
      "cs: CSI2 sess: 7 run: 6\n",
      "cs: CSI2 sess: 7 run: 7\n",
      "cs: CSI2 sess: 7 run: 8\n",
      "cs: CSI2 sess: 7 run: 9\n",
      "cs: CSI2 sess: 8 run: 1\n",
      "cs: CSI2 sess: 8 run: 2\n",
      "cs: CSI2 sess: 8 run: 3\n",
      "cs: CSI2 sess: 8 run: 4\n",
      "cs: CSI2 sess: 8 run: 5\n",
      "cs: CSI2 sess: 8 run: 6\n",
      "cs: CSI2 sess: 8 run: 7\n",
      "cs: CSI2 sess: 8 run: 8\n",
      "cs: CSI2 sess: 8 run: 9\n",
      "cs: CSI2 sess: 9 run: 1\n",
      "cs: CSI2 sess: 9 run: 2\n",
      "cs: CSI2 sess: 9 run: 3\n",
      "cs: CSI2 sess: 9 run: 4\n",
      "cs: CSI2 sess: 9 run: 5\n",
      "cs: CSI2 sess: 9 run: 6\n",
      "cs: CSI2 sess: 9 run: 7\n",
      "cs: CSI2 sess: 9 run: 8\n",
      "cs: CSI2 sess: 9 run: 9\n",
      "cs: CSI2 sess: 10 run: 1\n",
      "cs: CSI2 sess: 10 run: 2\n",
      "cs: CSI2 sess: 10 run: 3\n",
      "cs: CSI2 sess: 10 run: 4\n",
      "cs: CSI2 sess: 10 run: 5\n",
      "cs: CSI2 sess: 10 run: 6\n",
      "cs: CSI2 sess: 10 run: 7\n",
      "cs: CSI2 sess: 10 run: 8\n",
      "cs: CSI2 sess: 10 run: 9\n",
      "cs: CSI2 sess: 11 run: 1\n",
      "cs: CSI2 sess: 11 run: 2\n",
      "cs: CSI2 sess: 11 run: 3\n",
      "cs: CSI2 sess: 11 run: 4\n",
      "cs: CSI2 sess: 11 run: 5\n",
      "cs: CSI2 sess: 11 run: 6\n",
      "cs: CSI2 sess: 11 run: 7\n",
      "cs: CSI2 sess: 11 run: 8\n",
      "cs: CSI2 sess: 11 run: 9\n",
      "cs: CSI2 sess: 11 run: 10\n",
      "cs: CSI2 sess: 12 run: 1\n",
      "cs: CSI2 sess: 12 run: 2\n",
      "cs: CSI2 sess: 12 run: 3\n",
      "cs: CSI2 sess: 12 run: 4\n",
      "cs: CSI2 sess: 12 run: 5\n",
      "cs: CSI2 sess: 12 run: 6\n",
      "cs: CSI2 sess: 12 run: 7\n",
      "cs: CSI2 sess: 12 run: 8\n",
      "cs: CSI2 sess: 12 run: 9\n",
      "cs: CSI2 sess: 12 run: 10\n",
      "cs: CSI2 sess: 13 run: 1\n",
      "cs: CSI2 sess: 13 run: 2\n",
      "cs: CSI2 sess: 13 run: 3\n",
      "cs: CSI2 sess: 13 run: 4\n",
      "cs: CSI2 sess: 13 run: 5\n",
      "cs: CSI2 sess: 13 run: 6\n",
      "cs: CSI2 sess: 13 run: 7\n",
      "cs: CSI2 sess: 13 run: 8\n",
      "cs: CSI2 sess: 13 run: 9\n",
      "cs: CSI2 sess: 14 run: 1\n",
      "cs: CSI2 sess: 14 run: 2\n",
      "cs: CSI2 sess: 14 run: 3\n",
      "cs: CSI2 sess: 14 run: 4\n",
      "cs: CSI2 sess: 14 run: 5\n",
      "cs: CSI2 sess: 14 run: 6\n",
      "cs: CSI2 sess: 14 run: 7\n",
      "cs: CSI2 sess: 14 run: 8\n",
      "cs: CSI2 sess: 14 run: 9\n",
      "cs: CSI2 sess: 14 run: 10\n",
      "cs: CSI2 sess: 15 run: 1\n",
      "cs: CSI2 sess: 15 run: 2\n",
      "cs: CSI2 sess: 15 run: 3\n",
      "cs: CSI2 sess: 15 run: 4\n",
      "cs: CSI2 sess: 15 run: 5\n",
      "cs: CSI2 sess: 15 run: 6\n",
      "cs: CSI2 sess: 15 run: 7\n",
      "cs: CSI2 sess: 15 run: 8\n",
      "cs: CSI2 sess: 15 run: 9\n",
      "\n",
      "cs: CSI3 sess: 1 run: 1\n",
      "cs: CSI3 sess: 1 run: 2\n",
      "cs: CSI3 sess: 1 run: 3\n",
      "cs: CSI3 sess: 1 run: 4\n",
      "cs: CSI3 sess: 1 run: 5\n",
      "cs: CSI3 sess: 1 run: 6\n",
      "cs: CSI3 sess: 1 run: 7\n",
      "cs: CSI3 sess: 1 run: 8\n",
      "cs: CSI3 sess: 1 run: 9\n",
      "cs: CSI3 sess: 1 run: 10\n",
      "cs: CSI3 sess: 2 run: 1\n",
      "cs: CSI3 sess: 2 run: 2\n",
      "cs: CSI3 sess: 2 run: 3\n",
      "cs: CSI3 sess: 2 run: 4\n",
      "cs: CSI3 sess: 2 run: 5\n",
      "cs: CSI3 sess: 2 run: 6\n",
      "cs: CSI3 sess: 2 run: 7\n",
      "cs: CSI3 sess: 2 run: 8\n",
      "cs: CSI3 sess: 2 run: 9\n",
      "cs: CSI3 sess: 3 run: 1\n",
      "cs: CSI3 sess: 3 run: 2\n",
      "cs: CSI3 sess: 3 run: 3\n",
      "cs: CSI3 sess: 3 run: 4\n",
      "cs: CSI3 sess: 3 run: 5\n",
      "cs: CSI3 sess: 3 run: 6\n",
      "cs: CSI3 sess: 3 run: 7\n",
      "cs: CSI3 sess: 3 run: 8\n",
      "cs: CSI3 sess: 3 run: 9\n",
      "cs: CSI3 sess: 3 run: 10\n",
      "cs: CSI3 sess: 4 run: 1\n",
      "cs: CSI3 sess: 4 run: 2\n",
      "cs: CSI3 sess: 4 run: 3\n",
      "cs: CSI3 sess: 4 run: 4\n",
      "cs: CSI3 sess: 4 run: 5\n",
      "cs: CSI3 sess: 4 run: 6\n",
      "cs: CSI3 sess: 4 run: 7\n",
      "cs: CSI3 sess: 4 run: 8\n",
      "cs: CSI3 sess: 4 run: 9\n",
      "cs: CSI3 sess: 5 run: 1\n",
      "cs: CSI3 sess: 5 run: 2\n",
      "cs: CSI3 sess: 5 run: 3\n",
      "cs: CSI3 sess: 5 run: 4\n",
      "cs: CSI3 sess: 5 run: 5\n",
      "cs: CSI3 sess: 5 run: 6\n",
      "cs: CSI3 sess: 5 run: 7\n",
      "cs: CSI3 sess: 5 run: 8\n",
      "cs: CSI3 sess: 5 run: 9\n",
      "cs: CSI3 sess: 5 run: 10\n",
      "cs: CSI3 sess: 6 run: 1\n",
      "cs: CSI3 sess: 6 run: 2\n",
      "cs: CSI3 sess: 6 run: 3\n",
      "cs: CSI3 sess: 6 run: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs: CSI3 sess: 6 run: 5\n",
      "cs: CSI3 sess: 6 run: 6\n",
      "cs: CSI3 sess: 6 run: 7\n",
      "cs: CSI3 sess: 6 run: 8\n",
      "cs: CSI3 sess: 6 run: 9\n",
      "cs: CSI3 sess: 6 run: 10\n",
      "cs: CSI3 sess: 7 run: 1\n",
      "cs: CSI3 sess: 7 run: 2\n",
      "cs: CSI3 sess: 7 run: 3\n",
      "cs: CSI3 sess: 7 run: 4\n",
      "cs: CSI3 sess: 7 run: 5\n",
      "cs: CSI3 sess: 7 run: 6\n",
      "cs: CSI3 sess: 7 run: 7\n",
      "cs: CSI3 sess: 7 run: 8\n",
      "cs: CSI3 sess: 7 run: 9\n",
      "cs: CSI3 sess: 7 run: 10\n",
      "cs: CSI3 sess: 8 run: 1\n",
      "cs: CSI3 sess: 8 run: 2\n",
      "cs: CSI3 sess: 8 run: 3\n",
      "cs: CSI3 sess: 8 run: 4\n",
      "cs: CSI3 sess: 8 run: 5\n",
      "cs: CSI3 sess: 8 run: 6\n",
      "cs: CSI3 sess: 8 run: 7\n",
      "cs: CSI3 sess: 8 run: 8\n",
      "cs: CSI3 sess: 8 run: 9\n",
      "cs: CSI3 sess: 9 run: 1\n",
      "cs: CSI3 sess: 9 run: 2\n",
      "cs: CSI3 sess: 9 run: 3\n",
      "cs: CSI3 sess: 9 run: 4\n",
      "cs: CSI3 sess: 9 run: 5\n",
      "cs: CSI3 sess: 9 run: 6\n",
      "cs: CSI3 sess: 9 run: 7\n",
      "cs: CSI3 sess: 9 run: 8\n",
      "cs: CSI3 sess: 9 run: 9\n",
      "cs: CSI3 sess: 10 run: 1\n",
      "cs: CSI3 sess: 10 run: 2\n",
      "cs: CSI3 sess: 10 run: 3\n",
      "cs: CSI3 sess: 10 run: 4\n",
      "cs: CSI3 sess: 10 run: 5\n",
      "cs: CSI3 sess: 10 run: 6\n",
      "cs: CSI3 sess: 10 run: 7\n",
      "cs: CSI3 sess: 10 run: 8\n",
      "cs: CSI3 sess: 10 run: 9\n",
      "cs: CSI3 sess: 11 run: 1\n",
      "cs: CSI3 sess: 11 run: 2\n",
      "cs: CSI3 sess: 11 run: 3\n",
      "cs: CSI3 sess: 11 run: 4\n",
      "cs: CSI3 sess: 11 run: 5\n",
      "cs: CSI3 sess: 11 run: 6\n",
      "cs: CSI3 sess: 11 run: 7\n",
      "cs: CSI3 sess: 11 run: 8\n",
      "cs: CSI3 sess: 11 run: 9\n",
      "cs: CSI3 sess: 11 run: 10\n",
      "cs: CSI3 sess: 12 run: 1\n",
      "cs: CSI3 sess: 12 run: 2\n",
      "cs: CSI3 sess: 12 run: 3\n",
      "cs: CSI3 sess: 12 run: 4\n",
      "cs: CSI3 sess: 12 run: 5\n",
      "cs: CSI3 sess: 12 run: 6\n",
      "cs: CSI3 sess: 12 run: 7\n",
      "cs: CSI3 sess: 12 run: 8\n",
      "cs: CSI3 sess: 12 run: 9\n",
      "cs: CSI3 sess: 13 run: 1\n",
      "cs: CSI3 sess: 13 run: 2\n",
      "cs: CSI3 sess: 13 run: 3\n",
      "cs: CSI3 sess: 13 run: 4\n",
      "cs: CSI3 sess: 13 run: 5\n",
      "cs: CSI3 sess: 13 run: 6\n",
      "cs: CSI3 sess: 13 run: 7\n",
      "cs: CSI3 sess: 13 run: 8\n",
      "cs: CSI3 sess: 13 run: 9\n",
      "cs: CSI3 sess: 14 run: 1\n",
      "cs: CSI3 sess: 14 run: 2\n",
      "cs: CSI3 sess: 14 run: 3\n",
      "cs: CSI3 sess: 14 run: 4\n",
      "cs: CSI3 sess: 14 run: 5\n",
      "cs: CSI3 sess: 14 run: 6\n",
      "cs: CSI3 sess: 14 run: 7\n",
      "cs: CSI3 sess: 14 run: 8\n",
      "cs: CSI3 sess: 14 run: 9\n",
      "cs: CSI3 sess: 15 run: 1\n",
      "cs: CSI3 sess: 15 run: 2\n",
      "cs: CSI3 sess: 15 run: 3\n",
      "cs: CSI3 sess: 15 run: 4\n",
      "cs: CSI3 sess: 15 run: 5\n",
      "cs: CSI3 sess: 15 run: 6\n",
      "cs: CSI3 sess: 15 run: 7\n",
      "cs: CSI3 sess: 15 run: 8\n",
      "cs: CSI3 sess: 15 run: 9\n",
      "cs: CSI3 sess: 15 run: 10\n",
      "\n",
      "cs: CSI4 sess: 1 run: 1\n",
      "cs: CSI4 sess: 1 run: 2\n",
      "cs: CSI4 sess: 1 run: 3\n",
      "cs: CSI4 sess: 1 run: 4\n",
      "cs: CSI4 sess: 1 run: 5\n",
      "cs: CSI4 sess: 1 run: 6\n",
      "cs: CSI4 sess: 1 run: 7\n",
      "cs: CSI4 sess: 1 run: 8\n",
      "cs: CSI4 sess: 1 run: 9\n",
      "cs: CSI4 sess: 1 run: 10\n",
      "cs: CSI4 sess: 2 run: 1\n",
      "cs: CSI4 sess: 2 run: 2\n",
      "cs: CSI4 sess: 2 run: 3\n",
      "cs: CSI4 sess: 2 run: 4\n",
      "cs: CSI4 sess: 2 run: 5\n",
      "cs: CSI4 sess: 2 run: 6\n",
      "cs: CSI4 sess: 2 run: 7\n",
      "cs: CSI4 sess: 2 run: 8\n",
      "cs: CSI4 sess: 2 run: 9\n",
      "cs: CSI4 sess: 3 run: 1\n",
      "cs: CSI4 sess: 3 run: 2\n",
      "cs: CSI4 sess: 3 run: 3\n",
      "cs: CSI4 sess: 3 run: 4\n",
      "cs: CSI4 sess: 3 run: 5\n",
      "cs: CSI4 sess: 3 run: 6\n",
      "cs: CSI4 sess: 3 run: 7\n",
      "cs: CSI4 sess: 3 run: 8\n",
      "cs: CSI4 sess: 3 run: 9\n",
      "cs: CSI4 sess: 4 run: 1\n",
      "cs: CSI4 sess: 4 run: 2\n",
      "cs: CSI4 sess: 4 run: 3\n",
      "cs: CSI4 sess: 4 run: 4\n",
      "cs: CSI4 sess: 4 run: 5\n",
      "cs: CSI4 sess: 4 run: 6\n",
      "cs: CSI4 sess: 4 run: 7\n",
      "cs: CSI4 sess: 4 run: 8\n",
      "cs: CSI4 sess: 4 run: 9\n",
      "cs: CSI4 sess: 4 run: 10\n",
      "cs: CSI4 sess: 5 run: 1\n",
      "cs: CSI4 sess: 5 run: 2\n",
      "cs: CSI4 sess: 5 run: 3\n",
      "cs: CSI4 sess: 5 run: 4\n",
      "cs: CSI4 sess: 5 run: 5\n",
      "cs: CSI4 sess: 5 run: 6\n",
      "cs: CSI4 sess: 5 run: 7\n",
      "cs: CSI4 sess: 5 run: 8\n",
      "cs: CSI4 sess: 5 run: 9\n",
      "cs: CSI4 sess: 6 run: 1\n",
      "cs: CSI4 sess: 6 run: 2\n",
      "cs: CSI4 sess: 6 run: 3\n",
      "cs: CSI4 sess: 6 run: 4\n",
      "cs: CSI4 sess: 6 run: 5\n",
      "cs: CSI4 sess: 6 run: 6\n",
      "cs: CSI4 sess: 6 run: 7\n",
      "cs: CSI4 sess: 6 run: 8\n",
      "cs: CSI4 sess: 6 run: 9\n",
      "cs: CSI4 sess: 7 run: 1\n",
      "cs: CSI4 sess: 7 run: 2\n",
      "cs: CSI4 sess: 7 run: 3\n",
      "cs: CSI4 sess: 7 run: 4\n",
      "cs: CSI4 sess: 7 run: 5\n",
      "cs: CSI4 sess: 7 run: 6\n",
      "cs: CSI4 sess: 7 run: 7\n",
      "cs: CSI4 sess: 7 run: 8\n",
      "cs: CSI4 sess: 7 run: 9\n",
      "cs: CSI4 sess: 8 run: 1\n",
      "cs: CSI4 sess: 8 run: 2\n",
      "cs: CSI4 sess: 8 run: 3\n",
      "cs: CSI4 sess: 8 run: 4\n",
      "cs: CSI4 sess: 8 run: 5\n",
      "cs: CSI4 sess: 8 run: 6\n",
      "cs: CSI4 sess: 8 run: 7\n",
      "cs: CSI4 sess: 8 run: 8\n",
      "cs: CSI4 sess: 8 run: 9\n",
      "cs: CSI4 sess: 9 run: 1\n",
      "cs: CSI4 sess: 9 run: 2\n",
      "cs: CSI4 sess: 9 run: 3\n",
      "cs: CSI4 sess: 9 run: 4\n",
      "cs: CSI4 sess: 9 run: 5\n",
      "cs: CSI4 sess: 9 run: 6\n",
      "cs: CSI4 sess: 9 run: 7\n",
      "cs: CSI4 sess: 9 run: 8\n",
      "cs: CSI4 sess: 9 run: 9\n",
      "cs: CSI4 sess: 9 run: 10\n",
      "19380\n"
     ]
    }
   ],
   "source": [
    "# Get a global list of images file path with index matching trained data\n",
    "\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "\n",
    "global_data = {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\", \"CSI4\"],\n",
    "        \"start_sess\": 0,\n",
    "        \"last_sess\": 16,\n",
    "        \"start_run\": 0,\n",
    "        \"last_run\": 15\n",
    "}\n",
    "\n",
    "gList = {}\n",
    "global_index = 0\n",
    "imagePathList = []\n",
    "blankImage = 'Blank'\n",
    "\n",
    "for participant in global_data['participant_list']:\n",
    "        print()\n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        gList[participant] = {}\n",
    "        for sNum in range(global_data['start_sess'], global_data['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            gList[participant][sSes] = {}\n",
    "            for rNum in range(global_data['start_run'], global_data['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                if not os.path.exists(dir_path):\n",
    "                    continue\n",
    "\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                if not os.path.exists(stimulusListFilename):\n",
    "                    continue\n",
    "                    \n",
    "                print(\"cs: %s sess: %s run: %s\" % (participant, sNum, rNum))\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    imageList = f.read().splitlines()\n",
    "                    gList[participant][sSes][sRun] = imageList\n",
    "                    #global_index += len(fileList)\n",
    "                    for imageFileName in imageList:\n",
    "                        for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                            currBaseDir = os.path.basename(currDir)\n",
    "                            for filename in fileList:\n",
    "                                if filename in imageFileName:\n",
    "                                    fullFilename = os.path.join(currDir, filename)\n",
    "                                    imagePathList.append(fullFilename)\n",
    "                                    #print(fullFilename)\n",
    "                                    break\n",
    "\n",
    "                #last index for no image\n",
    "                #global_index += 1\n",
    "                imagePathList.append(blankImage)\n",
    "\n",
    "                    \n",
    "#print(global_index)\n",
    "print(len(imagePathList))\n",
    "\n",
    "# Other way to get global index from events file. just ran this to verify above\n",
    "#from glob import glob\n",
    "#import pandas as pd\n",
    "#import os\n",
    "#\n",
    "#events_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "#iCount = 0\n",
    "#imageList = []\n",
    "#for subname in ['sub-CSI1', 'sub-CSI2', 'sub-CSI3', 'sub-CSI4']:\n",
    "#    for sNum in range(0, 20):\n",
    "#        ses = \"ses-%s\" % str(sNum).zfill(2)\n",
    "#        event_path = os.path.join(events_dir,subname,ses,'func','*run*' + '_events.tsv')\n",
    "#        event = glob(event_path)\n",
    "#        if not event:\n",
    "#            continue\n",
    "#\n",
    "#        event_file = glob(os.path.join(events_dir,subname,ses,'func','*' + 'run*' + '_events.tsv'))\n",
    "#        for ev in event_file:\n",
    "#            events = pd.read_csv(ev, sep = '\\t')\n",
    "#            for index, row in events.iterrows():\n",
    "#                iCount += 1\n",
    "#                imageList.append(row['ImgName'])\n",
    "#                #print(iCount)\n",
    "#\n",
    "#            # index for no image\n",
    "#            iCount += 1\n",
    "#            imageList.append('None')\n",
    "#\n",
    "#print(iCount)\n",
    "#print(len(imageList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "probs (Dense)                   (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,636,712\n",
      "Trainable params: 25,583,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = Model(inputs = base_model.input, outputs = base_model.get_layer('avg_pool').output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_activations = []\n",
    "x_images = []\n",
    "for imgFile in imagePathList:\n",
    "    if imgFile is blankImage:\n",
    "        x_images.append(blankImage)\n",
    "        continue\n",
    "\n",
    "    img = image.load_img(imgFile, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x_images.append(preprocess_input(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19380\n",
      "(1, 2048)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blank_array = np.zeros((1, 2048))\n",
    "image_activations = [model.predict(x) if x is not blankImage else blank_array for x in x_images]\n",
    "print(len(image_activations))\n",
    "print(image_activations[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 2048)\n"
     ]
    }
   ],
   "source": [
    "activations_all = np.concatenate(image_activations)\n",
    "print(activations_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageActivation = \"imageActivation\"\n",
    "activationFile = \"stActivations.npy\"\n",
    "!mkdir -p imageActivation\n",
    "imageActivationFile = os.path.join(imageActivation, activationFile)\n",
    "\n",
    "with open(imageActivationFile, 'wb') as f:\n",
    "    pickle.dump(activations_all, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 10, 1100)\n",
      "17442\n",
      "(19380, 2048)\n",
      "xtrain shape: (17442, 10, 1100)\n",
      "ytrain shape: (17442, 2048)\n",
      "xtest shape: (1938, 10, 1100)\n",
      "ytest shape: (1938, 2048)\n",
      "Train on 17442 samples, validate on 1938 samples\n",
      "Epoch 1/100\n",
      "17442/17442 [==============================] - 23s 1ms/sample - loss: 0.6775 - mean_squared_error: 0.6775 - val_loss: 0.6639 - val_mean_squared_error: 0.6639\n",
      "Epoch 2/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6667 - mean_squared_error: 0.6667 - val_loss: 0.6623 - val_mean_squared_error: 0.6623\n",
      "Epoch 3/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6639 - mean_squared_error: 0.6639 - val_loss: 0.6616 - val_mean_squared_error: 0.6616\n",
      "Epoch 4/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6617 - mean_squared_error: 0.6617 - val_loss: 0.6622 - val_mean_squared_error: 0.6622\n",
      "Epoch 5/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6590 - mean_squared_error: 0.6590 - val_loss: 0.6638 - val_mean_squared_error: 0.6638\n",
      "Epoch 6/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6554 - mean_squared_error: 0.6554 - val_loss: 0.6663 - val_mean_squared_error: 0.6663\n",
      "Epoch 7/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6510 - mean_squared_error: 0.6510 - val_loss: 0.6688 - val_mean_squared_error: 0.6688\n",
      "Epoch 8/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6459 - mean_squared_error: 0.6459 - val_loss: 0.6725 - val_mean_squared_error: 0.6725\n",
      "Epoch 9/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6406 - mean_squared_error: 0.6406 - val_loss: 0.6756 - val_mean_squared_error: 0.6756\n",
      "Epoch 10/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6354 - mean_squared_error: 0.6354 - val_loss: 0.6788 - val_mean_squared_error: 0.6788\n",
      "Epoch 11/100\n",
      "17442/17442 [==============================] - 21s 1ms/sample - loss: 0.6311 - mean_squared_error: 0.6311 - val_loss: 0.6820 - val_mean_squared_error: 0.6820\n",
      "Epoch 12/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6268 - mean_squared_error: 0.6268 - val_loss: 0.6857 - val_mean_squared_error: 0.6857\n",
      "Epoch 13/100\n",
      "17442/17442 [==============================] - 21s 1ms/sample - loss: 0.6229 - mean_squared_error: 0.6229 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 14/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6196 - mean_squared_error: 0.6196 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 15/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6163 - mean_squared_error: 0.6163 - val_loss: 0.6934 - val_mean_squared_error: 0.6934\n",
      "Epoch 16/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6137 - mean_squared_error: 0.6137 - val_loss: 0.6957 - val_mean_squared_error: 0.6957\n",
      "Epoch 17/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6112 - mean_squared_error: 0.6112 - val_loss: 0.6986 - val_mean_squared_error: 0.6986\n",
      "Epoch 18/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6086 - mean_squared_error: 0.6086 - val_loss: 0.7000 - val_mean_squared_error: 0.7000\n",
      "Epoch 19/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.6061 - mean_squared_error: 0.6061 - val_loss: 0.7025 - val_mean_squared_error: 0.7025\n",
      "Epoch 20/100\n",
      "17442/17442 [==============================] - 21s 1ms/sample - loss: 0.6040 - mean_squared_error: 0.6040 - val_loss: 0.7059 - val_mean_squared_error: 0.7059\n",
      "Epoch 21/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.6021 - mean_squared_error: 0.6021 - val_loss: 0.7060 - val_mean_squared_error: 0.7060\n",
      "Epoch 22/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5999 - mean_squared_error: 0.5999 - val_loss: 0.7064 - val_mean_squared_error: 0.7064\n",
      "Epoch 23/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5982 - mean_squared_error: 0.5982 - val_loss: 0.7091 - val_mean_squared_error: 0.7091\n",
      "Epoch 24/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5967 - mean_squared_error: 0.5967 - val_loss: 0.7104 - val_mean_squared_error: 0.7104\n",
      "Epoch 25/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5949 - mean_squared_error: 0.5949 - val_loss: 0.7124 - val_mean_squared_error: 0.7124\n",
      "Epoch 26/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5935 - mean_squared_error: 0.5935 - val_loss: 0.7131 - val_mean_squared_error: 0.7131\n",
      "Epoch 27/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5922 - mean_squared_error: 0.5922 - val_loss: 0.7141 - val_mean_squared_error: 0.7141\n",
      "Epoch 28/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5910 - mean_squared_error: 0.5910 - val_loss: 0.7169 - val_mean_squared_error: 0.7169\n",
      "Epoch 29/100\n",
      "17442/17442 [==============================] - 21s 1ms/sample - loss: 0.5896 - mean_squared_error: 0.5896 - val_loss: 0.7176 - val_mean_squared_error: 0.7176\n",
      "Epoch 30/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5882 - mean_squared_error: 0.5882 - val_loss: 0.7181 - val_mean_squared_error: 0.7181\n",
      "Epoch 31/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5870 - mean_squared_error: 0.5870 - val_loss: 0.7180 - val_mean_squared_error: 0.7180\n",
      "Epoch 32/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5861 - mean_squared_error: 0.5861 - val_loss: 0.7202 - val_mean_squared_error: 0.7202\n",
      "Epoch 33/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5851 - mean_squared_error: 0.5851 - val_loss: 0.7192 - val_mean_squared_error: 0.7192\n",
      "Epoch 34/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5841 - mean_squared_error: 0.5841 - val_loss: 0.7214 - val_mean_squared_error: 0.7214\n",
      "Epoch 35/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5829 - mean_squared_error: 0.5829 - val_loss: 0.7225 - val_mean_squared_error: 0.7225\n",
      "Epoch 36/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5824 - mean_squared_error: 0.5824 - val_loss: 0.7223 - val_mean_squared_error: 0.7223\n",
      "Epoch 37/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5814 - mean_squared_error: 0.5814 - val_loss: 0.7232 - val_mean_squared_error: 0.7232\n",
      "Epoch 38/100\n",
      "17442/17442 [==============================] - 21s 1ms/sample - loss: 0.5804 - mean_squared_error: 0.5804 - val_loss: 0.7249 - val_mean_squared_error: 0.7249\n",
      "Epoch 39/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5799 - mean_squared_error: 0.5799 - val_loss: 0.7262 - val_mean_squared_error: 0.7262\n",
      "Epoch 40/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5786 - mean_squared_error: 0.5786 - val_loss: 0.7248 - val_mean_squared_error: 0.7248\n",
      "Epoch 41/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5780 - mean_squared_error: 0.5780 - val_loss: 0.7266 - val_mean_squared_error: 0.7266\n",
      "Epoch 42/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5775 - mean_squared_error: 0.5775 - val_loss: 0.7272 - val_mean_squared_error: 0.7272\n",
      "Epoch 43/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5765 - mean_squared_error: 0.5765 - val_loss: 0.7289 - val_mean_squared_error: 0.7289\n",
      "Epoch 44/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5762 - mean_squared_error: 0.5762 - val_loss: 0.7285 - val_mean_squared_error: 0.7285\n",
      "Epoch 45/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5754 - mean_squared_error: 0.5754 - val_loss: 0.7289 - val_mean_squared_error: 0.7289\n",
      "Epoch 46/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5752 - mean_squared_error: 0.5752 - val_loss: 0.7292 - val_mean_squared_error: 0.7292\n",
      "Epoch 47/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5743 - mean_squared_error: 0.5743 - val_loss: 0.7302 - val_mean_squared_error: 0.7302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5736 - mean_squared_error: 0.5736 - val_loss: 0.7302 - val_mean_squared_error: 0.7302\n",
      "Epoch 49/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5730 - mean_squared_error: 0.5730 - val_loss: 0.7314 - val_mean_squared_error: 0.7314\n",
      "Epoch 50/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5724 - mean_squared_error: 0.5724 - val_loss: 0.7318 - val_mean_squared_error: 0.7318\n",
      "Epoch 51/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5721 - mean_squared_error: 0.5721 - val_loss: 0.7318 - val_mean_squared_error: 0.7318\n",
      "Epoch 52/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5713 - mean_squared_error: 0.5713 - val_loss: 0.7317 - val_mean_squared_error: 0.7317\n",
      "Epoch 53/100\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.5709 - mean_squared_error: 0.5709 - val_loss: 0.7322 - val_mean_squared_error: 0.7322\n",
      "Epoch 54/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5705 - mean_squared_error: 0.5705 - val_loss: 0.7325 - val_mean_squared_error: 0.7325\n",
      "Epoch 55/100\n",
      "17442/17442 [==============================] - 20s 1ms/sample - loss: 0.5699 - mean_squared_error: 0.5699 - val_loss: 0.7337 - val_mean_squared_error: 0.7337\n",
      "Epoch 56/100\n",
      "10185/17442 [================>.............] - ETA: 7s - loss: 0.5686 - mean_squared_error: 0.5686"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9ff09c892554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_encoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train encoder .. fmri to image activation vector\n",
    "ROI_list = [\n",
    "    'X_LHPPA.npy', #(19380, 5, 100)\n",
    "    'X_RHLOC.npy', #(19380, 5, 170)\n",
    "    'X_LHLOC.npy', #(19380, 5, 130)\n",
    "    'X_RHEarlyVis.npy', #(19380, 5, 220)\n",
    "    'X_RHRSC.npy', #(19380, 5, 100)\n",
    "    'X_RHOPA.npy', #(19380, 5, 80)\n",
    "    'X_RHPPA.npy', #(19380, 5, 140)\n",
    "    'X_LHEarlyVis.npy', #(19380, 5, 190)\n",
    "    'X_LHRSC.npy', #(19380, 5, 30)\n",
    "    'X_LHOPA.npy', #(19380, 5, 70)\n",
    "]\n",
    "\n",
    "train_folder = '/home/ubuntu/cs230Project/dataset/traindata'\n",
    "ax_length = 5 * 220\n",
    "array_list = []\n",
    "for roc_file in ROI_list:\n",
    "    xt_file_path = os.path.join(train_folder, roc_file)\n",
    "    xtrain_n = np.load(xt_file_path)\n",
    "    # concatenate all 5 frame from each sample for now, also append all examples to 220 voxels\n",
    "    #xt = xtrain_n[:, 3, :]\n",
    "    # concatenate all 5 time frames together\n",
    "    xt = np.reshape(xtrain_n, (xtrain_n.shape[0], -1))\n",
    "    xt_pad = np.pad(xt, ((0, 0), (0, ax_length-xt.shape[1])), mode='constant', constant_values=0)\n",
    "    #print(xt_pad.shape)\n",
    "    array_list.append(xt_pad)\n",
    "\n",
    "\n",
    "x_stack = np.dstack(array_list)\n",
    "x_all = np.swapaxes(x_stack,1,2)\n",
    "#x_all = np.concatenate(array_list, axis=1)\n",
    "print(x_all.shape)\n",
    "\n",
    "num_samples = x_all.shape[0]\n",
    "#num_samples = 5000\n",
    "\n",
    "print(int(num_samples * 0.9))\n",
    "\n",
    "with open(imageActivationFile, 'rb') as f:\n",
    "    y_all = pickle.load(f)\n",
    "\n",
    "print(y_all.shape)\n",
    "\n",
    "callbacks = [TensorBoard(log_dir=f'./log/{i}')]\n",
    "\n",
    "##split data to train and dev\n",
    "x_train = x_all[0:int(num_samples * 0.9), :, :]\n",
    "y_train = y_all[0:int(num_samples * 0.9), :]\n",
    "x_test = x_all[int(num_samples * 0.9):, :, :]\n",
    "y_test = y_all[int(num_samples * 0.9):, :]\n",
    "#x_test = x_all[int(num_samples * 0.9):num_samples, :, :]\n",
    "#y_test = y_all[int(num_samples * 0.9):num_samples, :]\n",
    "\n",
    "print(\"xtrain shape: %s\" % str(x_train.shape))\n",
    "print(\"ytrain shape: %s\" % str(y_train.shape))\n",
    "print(\"xtest shape: %s\" % str(x_test.shape))\n",
    "print(\"ytest shape: %s\" % str(y_test.shape))\n",
    "\n",
    "def auto_encoder_lstm(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = LSTM(units = 128, return_sequences = True)(X_input)\n",
    "    #LSTM(128, dropout=0.2, recurrent_dropout=0.2)\n",
    "    #X = BatchNormalization()(X_input)\n",
    "    X = LSTM(128)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    #X = TimeDistributed(Dense(encoding_dim, activation = \"sigmoid\"))(X)\n",
    "    #X = Dense(64, activation = \"sigmoid\")(X)\n",
    "    X = Dense(encoding_dim, activation = \"sigmoid\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "assert x_train.shape[2] == ax_length\n",
    "assert x_train.shape[1] == len(ROI_list)\n",
    "\n",
    "encoder_model = auto_encoder_lstm((x_train.shape[1], x_train.shape[2]), y_train.shape[1])\n",
    "encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "train_history = encoder_model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d9df740d39b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "with open(imageActivationFile, 'wb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations from pretrained model and saved to file\n",
    "activations = {x: pretrained_model.predict(X_images[x]) for x in sorted(X_images.keys())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "\n",
    "   \n",
    "#data_split = {\n",
    "#    \"train\": {\n",
    "#        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "#        \"start_sess\": 1,\n",
    "#        \"last_sess\": 14,\n",
    "#        \"start_run\": 1,\n",
    "#        \"last_run\": 10\n",
    "#    },\n",
    "#    \"dev\": {\n",
    "#        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "#        \"start_sess\": 14,\n",
    "#        \"last_sess\": 15,\n",
    "#        \"start_run\": 1,\n",
    "#        \"last_run\": 10\n",
    "#    }\n",
    "#}\n",
    "\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 3,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 4\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI1\"],\n",
    "        \"start_sess\": 14,\n",
    "        \"last_sess\": 15,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 2\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "#print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "#print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "#print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "print('done')\n",
    "print(\"end time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(64, activation='tanh')(X)\n",
    "    #X = Dense(16, activation='tanh')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def dnn_gap_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = GlobalAveragePooling1D(data_format='channels_first')(X_input)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    #X = Activation('relu')(X)\n",
    "    #X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "\n",
    "def auto_encoder(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(encoding_dim, activation='relu')(X_input)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X_input)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "print(input_shape)\n",
    "#model = dnn_classifier(input_shape, num_classes)\n",
    "model = dnn_gap_classifier(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(data_split, data_type):\n",
    "    #every file has 35 feature vectors (one batch)   \n",
    "    x_images = data_split.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=20\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=4),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "#callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=(x_dev, y_dev))\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = load_model('weights.20.h5')\n",
    "new_model.summary()\n",
    "print(new_model.get_weights()[0].shape)\n",
    "print(new_model.get_weights()[1].shape)\n",
    "print(new_model.get_weights()[2].shape)\n",
    "print(new_model.get_weights()[3].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N=50\n",
    "arr1 = new_model.get_weights()[2][:,0]\n",
    "indices1 = np.argsort(arr1, axis=0)[-N:]\n",
    "arr2 = new_model.get_weights()[2][:,1]\n",
    "indices2 = np.argsort(arr2, axis=0)[-N:]\n",
    "arr3 = new_model.get_weights()[2][:,2]\n",
    "indices3 = np.argsort(arr3, axis=0)[-N:]\n",
    "#\n",
    "print(indices1)\n",
    "print(indices2)\n",
    "print(indices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "arr = new_model.get_weights()[0]\n",
    "N=20\n",
    "filter_select = []\n",
    "for index_list in [indices1, indices2, indices3]:\n",
    "    all_ind = []\n",
    "    for index in index_list:\n",
    "        indices = np.argsort(arr, axis=0)[-N:, index]\n",
    "        sort_ind = np.sort(indices, axis=-1)\n",
    "        all_ind.extend(list(sort_ind))\n",
    "        #print(sort_ind)\n",
    "        #plt.plot(sort_ind)\n",
    "    a_ind = [key for key,_ in Counter(all_ind).most_common()][0:10]\n",
    "    print(a_ind)\n",
    "    filter_select.extend([item for item in a_ind if item not in filter_select])\n",
    "    \n",
    "print(filter_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nibabel\n",
    "import nibabel as nib\n",
    "import re\n",
    "fmri_data_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "fmriRegex = re.compile(r'^(.*?)_sess(.*?)_run(.*?).npy$')\n",
    "\n",
    "# At the beginning and end of each run, a fixation cross was shown for 6 sec (3TORs) and\n",
    "# 12 sec (6TORs), respectively. hence stIndex goes from 3:-6\n",
    "# 37 images shows in each run >> 185 TOR\n",
    "# Each image was presented for 1 sec followed by a 9 sec fixation cross (5TORs)\n",
    "# For each stimuls, average assocated 5 TORs and map them\n",
    "def loadFmriData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69)\n",
    "    return x\n",
    "\n",
    "def loadFmriLstmData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69) > (37, 69, 106*106)\n",
    "    x = np.swapaxes(np.reshape(x, (37, -1, 69)), 1, 2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def loadFilterVector(file_path, filterNumList):\n",
    "    all_features = np.load(file_path, allow_pickle=True)\n",
    "    features = []\n",
    "    for filterNum in filterNumList:\n",
    "        features.append(all_features[:, filterNum, :].T)\n",
    "    \n",
    "    ft = np.asarray(features)\n",
    "    return ft.reshape(-1, 37).T\n",
    "\n",
    "filterNumList = [452, 209, 327, 377, 33, 16, 433, 19, 66, 467]\n",
    "data_split = x_images_path\n",
    "data_type = \"train\"\n",
    "x_images = data_split.get(data_type, None)\n",
    "for participant, sessDict in x_images.items():\n",
    "    for sess, runDict in sessDict.items():\n",
    "        for run in runDict.keys():\n",
    "            #fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % participant, \"sess\" \"_\".join([participant, sess, run]) + \".npy\")\n",
    "            feature_file_name = \"_\".join([participant, sess, run]) + \".npy\"\n",
    "            #sub-CSI3/ses-01/func\n",
    "            # sub-CSI3_ses-09_task-5000scenes_run-05_bold.nii.gz\n",
    "            match = fmriRegex.match(feature_file_name)\n",
    "            if match:\n",
    "                  fmri_file_name = \"sub-%s_ses-%s_task-5000scenes_run-%s_bold.nii.gz\" % ( match.group(1), match.group(2), match.group(3))\n",
    "                  fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % match.group(1), \"ses-%s\" % match.group(2), \"func\", fmri_file_name)\n",
    "                  print(fmri_data_path)\n",
    "                \n",
    "            feature_vector_path= os.path.join(stimuli_features_dir, feature_file_name)\n",
    "            X = loadFmriLstmData(fmri_data_path)\n",
    "            Y = loadFilterVector(feature_vector_path, filterNumList)\n",
    "            print(X.shape)\n",
    "            print(Y.shape)\n",
    "            break\n",
    "\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROI_list = [\n",
    "    'X_LHPPA.npy', #(19380, 5, 100)\n",
    "    'X_RHLOC.npy', #(19380, 5, 170)\n",
    "    'X_LHLOC.npy', #(19380, 5, 130)\n",
    "    'X_RHEarlyVis.npy', #(19380, 5, 220)\n",
    "    'X_RHRSC.npy', #(19380, 5, 100)\n",
    "    'X_RHOPA.npy', #(19380, 5, 80)\n",
    "    'X_RHPPA.npy', #(19380, 5, 140)\n",
    "    'X_LHEarlyVis.npy', #(19380, 5, 190)\n",
    "    'X_LHRSC.npy', #(19380, 5, 30)\n",
    "    'X_LHOPA.npy', #(19380, 5, 70)\n",
    "]\n",
    "\n",
    "#x_all (19380, 5, 1230)\n",
    "#y_all shape (19380, 17)\n",
    "\n",
    "#one way is to concatenate last dimenesion and just use 5 time series\n",
    "# so lstm input would be \n",
    "# other way is to train each roi seperately to encode to feature vector. and then inout feature vectoers to classify\n",
    "# or input roi as LSTM nodes to get one feature vector\n",
    "\n",
    "train_folder = '/home/ubuntu/cs230Project/dataset/traindata'\n",
    "array_list = []\n",
    "for roc_file in ROI_list:\n",
    "    xt_file_path = os.path.join(train_folder, roc_file)\n",
    "    xtrain_n = np.load(xt_file_path)\n",
    "    array_list.append(xtrain_n)\n",
    "    #print(xtrain_n.shape)\n",
    "\n",
    "#all_x = np.asarray(array_list)\n",
    "#print(all_x.shape)\n",
    "x_all = np.dstack(array_list)\n",
    "print(x_all.shape)\n",
    "\n",
    "yt_file_path = os.path.join(train_folder, 'Yreal_all.npy')\n",
    "y_all = np.load(yt_file_path)\n",
    "\n",
    "num_classes = y_all.shape[1]\n",
    "\n",
    "x_train = x_all[0:18380, :, :]\n",
    "y_train = y_all[0:18380, :]\n",
    "x_test = x_all[18380:, :, :]\n",
    "y_test = y_all[18380:, :]\n",
    "\n",
    "def classifer_lstm(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = LSTM(512, dropout=0.2)(X_input)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation = \"softmax\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "Tx = x_train.shape[1]\n",
    "Voxels = x_train.shape[2]\n",
    "classifier_lstm = classifer_lstm((Tx, Voxels), num_classes)\n",
    "\n",
    "EPOCHS=100\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "#callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "classifier_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = classifier_lstm.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "train_history = classifier_lstm.fit(x=x_train, y=y_train, epochs=EPOCHS, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_list[0].shape)\n",
    "print(array_list[1].shape)\n",
    "test = np.dstack((array_list[0],array_list[1]))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single layer neural network (one network per filter) to map fmri data to above filters\n",
    "# input: X of shape (106, 106, 69, 194) \n",
    "# output: Y of shape (144, 1) image features on specific filters\n",
    "fmriRegex = re.compile(r'^(.*?)_sess(.*?)_run(.*?).npy$')\n",
    "\n",
    "fmri_data_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "\n",
    "def loadFilterVector(file_path, filterNumList):\n",
    "    all_features = np.load(file_path, allow_pickle=True)\n",
    "    #features = []\n",
    "    #for filterNum in filterNumList:\n",
    "    #    features.append(all_features[:, filterNum, :].T)\n",
    "    \n",
    "    #ft = np.asarray(features)\n",
    "    #return ft.reshape(-1, 37).T\n",
    "    #return all_features[:, 377, :]\n",
    "    feat_sel = all_features[:, filterNumList, :]\n",
    "    sel_shape = feat_sel.shape[0]\n",
    "    return feat_sel.reshape(sel_shape, -1)\n",
    "\n",
    "# At the beginning and end of each run, a fixation cross was shown for 6 sec (3TORs) and\n",
    "# 12 sec (6TORs), respectively. hence stIndex goes from 3:-6\n",
    "# 37 images shows in each run >> 185 TOR\n",
    "# Each image was presented for 1 sec followed by a 9 sec fixation cross (5TORs)\n",
    "# For each stimuls, average assocated 5 TORs and map them\n",
    "def loadFmriData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4, img_data.shape[-1] - 5, 5):\n",
    "        #x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "        x_train.append((img_data[:,:,:,stIndex+3]))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    return x\n",
    "\n",
    "def loadFmriLstmData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69) > (37, 69, 106*106)\n",
    "    x = np.swapaxes(np.reshape(x, (37, -1, 69)), 1, 2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def featureVectorLoader(data_split, data_type, filterNum):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = data_split.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    #fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % participant, \"sess\" \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    feature_file_name = \"_\".join([participant, sess, run]) + \".npy\"\n",
    "                    #sub-CSI3/ses-01/func\n",
    "                    # sub-CSI3_ses-09_task-5000scenes_run-05_bold.nii.gz\n",
    "                    match = fmriRegex.match(feature_file_name)\n",
    "                    if match:\n",
    "                        fmri_file_name = \"sub-%s_ses-%s_task-5000scenes_run-%s_bold.nii.gz\" % ( match.group(1), match.group(2), match.group(3))\n",
    "                        fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % match.group(1), \"ses-%s\" % match.group(2), \"func\", fmri_file_name)\n",
    "                \n",
    "                    feature_vector_path= os.path.join(stimuli_features_dir, feature_file_name)\n",
    "                    X = loadFmriLstmData(fmri_data_path)\n",
    "                    Y = loadFilterVector(feature_vector_path, filterNum)\n",
    "                    yield (X,Y)\n",
    "\n",
    "\n",
    "# for each y, we have \n",
    "def auto_encoder(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = Conv2D(2, (5,5), activation='relu')(X_input)\n",
    "    #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    #X = Dropout(0.25)(X)\n",
    "    X = Conv2D(4, (1,1), activation='tanh')(X_input)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(encoding_dim, activation='relu')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder')\n",
    "    return model\n",
    "\n",
    "# for each y, we have \n",
    "def auto_encoder_lstm(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = LSTM(units = 128, return_sequences = True)(X_input)\n",
    "    #LSTM(128, dropout=0.2, recurrent_dropout=0.2)\n",
    "    X = LSTM(128)(X_input)\n",
    "    #X = TimeDistributed(Dense(encoding_dim, activation = \"sigmoid\"))(X)\n",
    "    X = Dense(encoding_dim, activation = \"sigmoid\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "EPOCHS = 100\n",
    "filterNumList = [452, 209, 327, 377, 33, 16, 433, 19, 66, 467]\n",
    "#filterNumList = [377]\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "encoder_model = auto_encoder_lstm((69, 106*106), len(filterNumList)*144)\n",
    "#encoder_model = auto_encoder((106,106,69), len(filterNumList)*144)\n",
    "#cosine_proximity\n",
    "#encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "train_history = encoder_model.fit_generator(featureVectorLoader(x_images_path, \"train\", filterNumList), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    validation_data=featureVectorLoader(x_images_path, \"dev\", filterNumList),\n",
    "                                    validation_steps=validation_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(512, activation='tanh')(X)\n",
    "    X = Dense(128, activation='tanh')(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "input_shape = x_content.shape[1:]\n",
    "model2 = dnn_classifier(input_shape, num_classes)\n",
    "\n",
    "EPOCHS=100\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "#                                    validation_steps=validation_steps) \n",
    "train_history = model2.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
