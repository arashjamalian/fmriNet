{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulusDirPath: images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli\n",
      "(1, 37)\n",
      "(1, 37)\n",
      "37\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 3,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 4\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI3\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 3,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 4\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 02:22:52.972341 140181833897728 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 02:23:00.625792 140181833897728 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 444\n",
      "Total number of dev examples: 222\n",
      "steps_per_epoch: 12\n",
      "Epoch 1/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 4.6718 - acc: 0.4963\n",
      "Epoch 00001: saving model to weights.01.h5\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 4.3984 - acc: 0.5045 - val_loss: 0.2446 - val_acc: 0.9054\n",
      "Epoch 2/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.5513 - acc: 0.7838\n",
      "Epoch 00002: saving model to weights.02.h5\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.5497 - acc: 0.7815 - val_loss: 0.3375 - val_acc: 0.9009\n",
      "Epoch 3/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.2703 - acc: 0.8845\n",
      "Epoch 00003: saving model to weights.03.h5\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.2717 - acc: 0.8851 - val_loss: 0.1499 - val_acc: 0.9595\n",
      "Epoch 4/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.1260 - acc: 0.9435\n",
      "Epoch 00004: saving model to weights.04.h5\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.1305 - acc: 0.9414 - val_loss: 0.0490 - val_acc: 0.9775\n",
      "Epoch 5/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0769 - acc: 0.9681\n",
      "Epoch 00005: saving model to weights.05.h5\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.0822 - acc: 0.9640 - val_loss: 0.0131 - val_acc: 0.9955\n",
      "Epoch 6/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0429 - acc: 0.9828\n",
      "Epoch 00006: saving model to weights.06.h5\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 0.0460 - acc: 0.9820 - val_loss: 0.0134 - val_acc: 0.9955\n",
      "Epoch 7/15\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0460 - acc: 0.9803\n",
      "Epoch 00007: saving model to weights.07.h5\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0506 - acc: 0.9775 - val_loss: 0.0198 - val_acc: 0.9910\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(x_images_path, data_type):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = x_images_path.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=15\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"train\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
