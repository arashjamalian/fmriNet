{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 35)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "sCS = \"CSI2\"\n",
    "sesNum = \"01\"\n",
    "runNum = \"02\"\n",
    "sSes = \"sess\" + sesNum\n",
    "sRun = \"run\" + runNum\n",
    "\n",
    "stList = {}\n",
    "last_sess = 2 # 15\n",
    "last_run = 9 # 9\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for sNum in range(1, last_sess):\n",
    "    sSes = \"sess\" + str(sNum).zfill(2)\n",
    "    stList[sSes] = {}\n",
    "    for rNum in range(1, last_run):\n",
    "        sRun = \"run\" + str(rNum).zfill(2)\n",
    "        stimulusDirPath = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",sCS, sCS + \"_\" + sSes)\n",
    "        #print(stimulusDirPath)\n",
    "        stimulusListFilename = os.path.join(stimulusDirPath, \"_\".join([sCS, sSes, sRun]) + \".txt\")\n",
    "        #print(stimulusListFilename)\n",
    "        with open(stimulusListFilename) as f:\n",
    "            stList[sSes][sRun] = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Takes ~1 min\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "\n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "for sess in stList:\n",
    "    x_images_path[sess] = {}\n",
    "    y_labels[sess] = {}\n",
    "    for run in stList[sess]:\n",
    "        x_images_path[sess][run] = []\n",
    "        y_labels[sess][run] = []\n",
    "        #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "        labelList = []\n",
    "        for imageFileName in stList[sess][run]:\n",
    "            for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                currBaseDir = os.path.basename(currDir)\n",
    "                for filename in fileList:\n",
    "                    if filename == imageFileName:\n",
    "                        fullFilename = os.path.join(currDir, filename)\n",
    "                        #print(currDir)\n",
    "                        x_images_path[sess][run].append(fullFilename)\n",
    "                        # using directory path to determine class\n",
    "                        labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "\n",
    "        y_labels[sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "#print(x_images_path)\n",
    "print(y_labels['sess01']['run01'].shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 35)\n",
      "[[0 0 1 1 1 2 1 0 1 0 1 2 1 0 0 2 1 2 1 0 0 2 0 1 1 0 2 2 0 1 1 0 1 0 0]]\n",
      "35\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#print(x_images_path)\n",
    "print(y_labels['sess01']['run01'].shape)\n",
    "print(y_labels['sess01']['run01'])\n",
    "print(len(x_images_path['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 00:57:15.117236 140608365524736 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: stimulifeatures/CSI2_sess01_run01.npy\n",
      "sess: sess01, run: run01\n"
     ]
    }
   ],
   "source": [
    "#print(x_images_path[\"sess04\"][\"run07\"])\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for sess in x_images_path:\n",
    "        for run in x_images_path[sess]:\n",
    "            #x_content = {sess: {run: []}}\n",
    "            file_path= os.path.join(stimuli_features_dir, \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "            print(\"file_path: %s\" % file_path)\n",
    "            print(\"sess: %s, run: %s\" %(sess, run))\n",
    "            contentList = []\n",
    "            for img_path in x_images_path[sess][run]:\n",
    "                #stImage = imread(cImage)\n",
    "                img = image.load_img(img_path, target_size=(375, 375))\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "                #print(\"img_path: %s\" % img_path)\n",
    "                #print('Input image shape:', x.shape)\n",
    "                #img_array = img_to_array(img)\n",
    "                #stImage = imageio.imread(img_path)\n",
    "                #print(\"img_path: %s\" % img_path)\n",
    "                #print(stImage.shape)\n",
    "                #stImage = reshape_and_normalize_image(stImage)\n",
    "                #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                ts.run(vmodel['input'].assign(x))\n",
    "                #a_C = sess.run(vmodel)\n",
    "                out = vmodel[content_layer]\n",
    "                contentOut = ts.run(out)\n",
    "                contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "            #x_content[sess][run] = np.asarray(contentList)\n",
    "            contentArray = np.asarray(contentList)\n",
    "            # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "            #print(x_content[sess][run].shape)\n",
    "            #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "\n",
    "            #np.save(file_path, x_content)\n",
    "            np.save(file_path, contentArray)\n",
    "            #del x_content\n",
    "\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "#x_content = np.load(file_path, allow_pickle=True).item()\n",
    "#print(x_content.get('sess01')['run01'].shape)\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "\n",
    "# load data\n",
    "#x_train = x_content.get('sess01')['run01']\n",
    "x_train = x_content\n",
    "y_train = np.transpose(y_labels['sess01']['run01'])\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run02.npy')\n",
    "#x_content = np.load(file_path, allow_pickle=True).item()\n",
    "#print(x_content.get('sess01')['run02'].shape)\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "#x_test = x_content.get('sess01')['run02']\n",
    "x_test = x_content\n",
    "y_test = np.transpose(y_labels['sess01']['run02'])\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# each batch will be of 35 feature vectors of size 512 x 144 (35, 512, 144)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "input_shape=[512, 144]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(x_images_path):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    while True:\n",
    "        for sess in x_images_path:\n",
    "            for run in x_images_path[sess]:\n",
    "                file_path= os.path.join(stimuli_features_dir, \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "                X = loadFeatureVector(file_path)\n",
    "                Y = utils.to_categorical(np.transpose(y_labels['sess01']['run02']))\n",
    "                yield (X,Y)\n",
    "            \n",
    "\n",
    "EPOCHS=5\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"here\")\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and test set\n",
    "# For milestone, first we try to train a classifier network using feature vectors from above\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "#concatenate all feature vectors from all sessions/runs\n",
    "x_all_list = []\n",
    "for (currDir, _, fileList) in os.walk(stimuli_features_dir):\n",
    "    currBaseDir = os.path.basename(currDir)\n",
    "    for filename in fileList:\n",
    "        fullFilename = os.path.join(currDir, filename)\n",
    "        print(fullFilename)\n",
    "        x_content = np.load(fullFilename, allow_pickle=True).item()\n",
    "        for sess in x_content:\n",
    "            for run in x_content[sess]:\n",
    "                x_all_list.extend(x_content[sess][run])\n",
    "\n",
    "print(x_all_list[0].shape)\n",
    "print(x_all_list[1].shape)\n",
    "print(x_all_list[43].shape)\n",
    "x_all = np.asarray(x_all_list)\n",
    "print(x_all.shape)\n",
    "\n",
    "#for sess in x_images_path:\n",
    "#    for run in x_images_path[sess]:\n",
    "#        file_path = os.path.join('stimulifeatures', \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "#        x_content = np.load(file_path)\n",
    "#        print(x_content.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for sess in x_images_path:\n",
    "#    for run in x_images_path[sess]:\n",
    "#        file_path = os.path.join('stimulifeatures', \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "#        x_content = np.load(file_path)\n",
    "#        print(x_content.keys())\n",
    "            \n",
    "\n",
    "#print(x_content['sess01']['run01'][0].shape)\n",
    "#print(x_content['sess01']['run01'][3].shape)\n",
    "#print(y_labels['sess01']['run01'].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create train and test test\n",
    "x_all = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# split train test set\n",
    "# del loaded data\n",
    "# assert on expected shape\n",
    "# assert x_train.shape == ()\n",
    "\n",
    "#K.set_image_date_format('channels_last')\n",
    "\n",
    "#mnist = tf.keras.datasets.mnist\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_val, x_test = np.split(x_test, 2)\n",
    "#y_val, y_test = np.split(y_test, 2)\n",
    "\n",
    "#x_train, x_val, x_test = x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "#print(x_train.shape)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train_orig), (x_test_orig, y_test_orig) = cifar10.load_data()\n",
    "x_val, x_test = np.split(x_test_orig, 2)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = utils.to_categorical(y_train_orig, num_classes)\n",
    "y_test_1 = utils.to_categorical(y_test_orig, num_classes)\n",
    "\n",
    "#y_val, y_test = np.split(y_test_1, 2)\n",
    "del x_test_orig\n",
    "del y_test_orig\n",
    "del y_train_orig\n",
    "del y_test_1\n",
    "\n",
    "#x_train, x_val, x_test = x_train / 255.0, x_val/255.0, x_train/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(num_classes),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def classifer_cnn(input, name):\n",
    "#    X = \n",
    "#    X = Conv2D()\n",
    "#    return Model(input, X, name=name)\n",
    "#input_shape =(28, 28, 3)\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "#                           activation='relu',\n",
    "#                           input_shape=input_shape),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(10, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=input_shape))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "!mkdir -p models\n",
    "#%notebook -e models/{VERSION}.ipynb\n",
    "\n",
    "#K.clear_session()\n",
    "#K.set_session(tf.Session(config=))\n",
    "#model = classifer_cnn(x_in, 'classifer_cnn')\n",
    "#model.save(f'models/{VERSION}.architecture.h5')\n",
    "#model.save('models/latest.architecture.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=40\n",
    "#K.set_image_date_format('channels_last')\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 10:\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "#callbacks\n",
    "#tensorboard = Tensorboard(log_di='logs/' + VERSION)\n",
    "#checkpoint_file_path = f'models/{VERSION}/{VERSION}.checkpoint.h5'\n",
    "checkpoint_file_path = 'weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_file_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2),\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                              patience=5, min_lr=0.001)\n",
    "#learning_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "#callbacks = [model_checkpoint, reduce_lr]\n",
    "#callbacks = [model_checkpoint, reduce_lr, learning_scheduler]\n",
    "#callbacks = [early_stopping, model_checkpoint]\n",
    "#callbacks = [early_stopping]\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=256, validation_data=(x_val, y_val))\n",
    "train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=256, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "#plotter.losses(history)\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n",
    "#initial_epoch += EPOCHS\n",
    "#model.save( f'models/{VERSION}.checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plotter.losses(history)\n",
    "initial_epoch += 13\n",
    "model.save( f'models/{VERSION}.checkpoint.h5')\n",
    "\n",
    "#initial_epoch += EPOCHS\n",
    "#model.save( f'models/{VERSION}/{VERSION}.checkpoint.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "my_image = imread(img_path)\n",
    "imshow(my_image)\n",
    "\n",
    "img = image.load_img(img_path, target_size=(32, 32))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sample = x_train[120]\n",
    "y_sample = y_train[120]\n",
    "print(x_sample.shape)\n",
    "imshow(x_sample)\n",
    "print(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
