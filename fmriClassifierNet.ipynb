{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulusDirPath: images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli\n",
      "(1, 37)\n",
      "(1, 37)\n",
      "37\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 4,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 9\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI3\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 3,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 5\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 03:25:36.973730 140389025142528 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1030 03:25:48.147598 140389025142528 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 1776\n",
      "Total number of dev examples: 296\n",
      "steps_per_epoch: 48\n",
      "Epoch 1/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 2.3132 - acc: 0.5394\n",
      "Epoch 00001: saving model to weights.01.h5\n",
      "48/48 [==============================] - 4s 73ms/step - loss: 2.2825 - acc: 0.5400 - val_loss: 0.5051 - val_acc: 0.7331\n",
      "Epoch 2/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.7510\n",
      "Epoch 00002: saving model to weights.02.h5\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 0.5436 - acc: 0.7506 - val_loss: 0.3130 - val_acc: 0.8784\n",
      "Epoch 3/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8315\n",
      "Epoch 00003: saving model to weights.03.h5\n",
      "48/48 [==============================] - 2s 50ms/step - loss: 0.3582 - acc: 0.8333 - val_loss: 0.2137 - val_acc: 0.9155\n",
      "Epoch 4/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.8781\n",
      "Epoch 00004: saving model to weights.04.h5\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.2713 - acc: 0.8801 - val_loss: 0.0957 - val_acc: 0.9865\n",
      "Epoch 5/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9270\n",
      "Epoch 00005: saving model to weights.05.h5\n",
      "48/48 [==============================] - 2s 50ms/step - loss: 0.1770 - acc: 0.9279 - val_loss: 0.0653 - val_acc: 0.9730\n",
      "Epoch 6/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9442\n",
      "Epoch 00006: saving model to weights.06.h5\n",
      "48/48 [==============================] - 2s 50ms/step - loss: 0.1415 - acc: 0.9448 - val_loss: 0.0253 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9597\n",
      "Epoch 00007: saving model to weights.07.h5\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.1118 - acc: 0.9589 - val_loss: 0.0269 - val_acc: 0.9966\n",
      "Epoch 8/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9724\n",
      "Epoch 00008: saving model to weights.08.h5\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.0866 - acc: 0.9724 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9712\n",
      "Epoch 00009: saving model to weights.09.h5\n",
      "48/48 [==============================] - 2s 51ms/step - loss: 0.0787 - acc: 0.9718 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9724\n",
      "Epoch 00010: saving model to weights.10.h5\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 0.0632 - acc: 0.9730 - val_loss: 0.0264 - val_acc: 0.9831\n",
      "Epoch 11/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9804\n",
      "Epoch 00011: saving model to weights.11.h5\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.0536 - acc: 0.9803 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9781\n",
      "Epoch 00012: saving model to weights.12.h5\n",
      "48/48 [==============================] - 2s 52ms/step - loss: 0.0575 - acc: 0.9786 - val_loss: 0.0124 - val_acc: 0.9966\n",
      "Epoch 13/15\n",
      "47/48 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9776\n",
      "Epoch 00013: saving model to weights.13.h5\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 0.0567 - acc: 0.9780 - val_loss: 0.0062 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(x_images_path, data_type):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = x_images_path.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=15\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"train\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (1, 512, 144)\n",
      "[[9.9750739e-01 2.4926958e-03 2.4027281e-08]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
