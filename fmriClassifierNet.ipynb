{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 37)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "sCS = \"CSI2\"\n",
    "sesNum = \"01\"\n",
    "runNum = \"02\"\n",
    "sSes = \"sess\" + sesNum\n",
    "sRun = \"run\" + runNum\n",
    "\n",
    "stList = {}\n",
    "last_sess = 3 # 15\n",
    "last_run = 9 # 9\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for sNum in range(1, last_sess):\n",
    "    sSes = \"sess\" + str(sNum).zfill(2)\n",
    "    stList[sSes] = {}\n",
    "    for rNum in range(1, last_run):\n",
    "        sRun = \"run\" + str(rNum).zfill(2)\n",
    "        stimulusDirPath = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",sCS, sCS + \"_\" + sSes)\n",
    "        #print(stimulusDirPath)\n",
    "        stimulusListFilename = os.path.join(stimulusDirPath, \"_\".join([sCS, sSes, sRun]) + \".txt\")\n",
    "        #print(stimulusListFilename)\n",
    "        with open(stimulusListFilename) as f:\n",
    "            stList[sSes][sRun] = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Takes ~1 min\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "\n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "for sess in stList:\n",
    "    x_images_path[sess] = {}\n",
    "    y_labels[sess] = {}\n",
    "    for run in stList[sess]:\n",
    "        x_images_path[sess][run] = []\n",
    "        y_labels[sess][run] = []\n",
    "        #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "        labelList = []\n",
    "        for imageFileName in stList[sess][run]:\n",
    "            for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                currBaseDir = os.path.basename(currDir)\n",
    "                for filename in fileList:\n",
    "                    if filename in imageFileName:\n",
    "                        fullFilename = os.path.join(currDir, filename)\n",
    "                        #print(currDir)\n",
    "                        x_images_path[sess][run].append(fullFilename)\n",
    "                        # using directory path to determine class\n",
    "                        labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                        break\n",
    "\n",
    "        y_labels[sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "#print(x_images_path)\n",
    "print(y_labels['sess01']['run01'].shape)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 37)\n",
      "(1, 37)\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(y_labels['sess01']['run01'].shape)\n",
    "print(y_labels['sess02']['run01'].shape)\n",
    "print(len(stList['sess02']['run01']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 37)\n",
      "[[0 0 1 1 1 1 2 1 0 1 1 0 1 2 1 0 0 2 1 2 1 0 0 2 0 1 1 0 2 2 0 1 1 0 1 0\n",
      "  0]]\n",
      "37\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#print(x_images_path)\n",
    "print(y_labels['sess01']['run01'].shape)\n",
    "print(y_labels['sess01']['run01'])\n",
    "print(len(x_images_path['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 03:01:29.776621 140607309653760 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: stimulifeatures/CSI2_sess01_run01.npy\n",
      "sess: sess01, run: run01\n",
      "file_path: stimulifeatures/CSI2_sess01_run02.npy\n",
      "sess: sess01, run: run02\n",
      "file_path: stimulifeatures/CSI2_sess01_run03.npy\n",
      "sess: sess01, run: run03\n",
      "file_path: stimulifeatures/CSI2_sess01_run04.npy\n",
      "sess: sess01, run: run04\n",
      "file_path: stimulifeatures/CSI2_sess01_run05.npy\n",
      "sess: sess01, run: run05\n",
      "file_path: stimulifeatures/CSI2_sess01_run06.npy\n",
      "sess: sess01, run: run06\n",
      "file_path: stimulifeatures/CSI2_sess01_run07.npy\n",
      "sess: sess01, run: run07\n",
      "file_path: stimulifeatures/CSI2_sess01_run08.npy\n",
      "sess: sess01, run: run08\n",
      "file_path: stimulifeatures/CSI2_sess02_run01.npy\n",
      "sess: sess02, run: run01\n",
      "file_path: stimulifeatures/CSI2_sess02_run02.npy\n",
      "sess: sess02, run: run02\n",
      "file_path: stimulifeatures/CSI2_sess02_run03.npy\n",
      "sess: sess02, run: run03\n",
      "file_path: stimulifeatures/CSI2_sess02_run04.npy\n",
      "sess: sess02, run: run04\n",
      "file_path: stimulifeatures/CSI2_sess02_run05.npy\n",
      "sess: sess02, run: run05\n",
      "file_path: stimulifeatures/CSI2_sess02_run06.npy\n",
      "sess: sess02, run: run06\n",
      "file_path: stimulifeatures/CSI2_sess02_run07.npy\n",
      "sess: sess02, run: run07\n",
      "file_path: stimulifeatures/CSI2_sess02_run08.npy\n",
      "sess: sess02, run: run08\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#print(x_images_path[\"sess04\"][\"run07\"])\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for sess in x_images_path:\n",
    "        for run in x_images_path[sess]:\n",
    "            #x_content = {sess: {run: []}}\n",
    "            file_path= os.path.join(stimuli_features_dir, \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "            print(\"file_path: %s\" % file_path)\n",
    "            print(\"sess: %s, run: %s\" %(sess, run))\n",
    "            contentList = []\n",
    "            for img_path in x_images_path[sess][run]:\n",
    "                #stImage = imread(cImage)\n",
    "                img = image.load_img(img_path, target_size=(375, 375))\n",
    "                x = image.img_to_array(img)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = preprocess_input(x)\n",
    "                #print(\"img_path: %s\" % img_path)\n",
    "                #print('Input image shape:', x.shape)\n",
    "                #img_array = img_to_array(img)\n",
    "                #stImage = imageio.imread(img_path)\n",
    "                #print(\"img_path: %s\" % img_path)\n",
    "                #print(stImage.shape)\n",
    "                #stImage = reshape_and_normalize_image(stImage)\n",
    "                #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                ts.run(vmodel['input'].assign(x))\n",
    "                #a_C = sess.run(vmodel)\n",
    "                out = vmodel[content_layer]\n",
    "                contentOut = ts.run(out)\n",
    "                contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "            #x_content[sess][run] = np.asarray(contentList)\n",
    "            contentArray = np.asarray(contentList)\n",
    "            # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "            #print(x_content[sess][run].shape)\n",
    "            #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "\n",
    "            #np.save(file_path, x_content)\n",
    "            np.save(file_path, contentArray)\n",
    "            #del x_content\n",
    "\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess02_run04.npy')\n",
    "#x_content = np.load(file_path, allow_pickle=True).item()\n",
    "#print(x_content.get('sess01')['run01'].shape)\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n",
      "(37, 512, 144)\n",
      "(37, 512, 144)\n",
      "(37, 3)\n",
      "(37, 3)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "#x_content = np.load(file_path, allow_pickle=True).item()\n",
    "#print(x_content.get('sess01')['run01'].shape)\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "\n",
    "# load data\n",
    "#x_train = x_content.get('sess01')['run01']\n",
    "x_train = x_content\n",
    "y_train = np.transpose(y_labels['sess01']['run01'])\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run02.npy')\n",
    "#x_content = np.load(file_path, allow_pickle=True).item()\n",
    "#print(x_content.get('sess01')['run02'].shape)\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "#x_test = x_content.get('sess01')['run02']\n",
    "x_test = x_content\n",
    "y_test = np.transpose(y_labels['sess01']['run02'])\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# each batch will be of 35 feature vectors of size 512 x 144 (35, 512, 144)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1029 03:25:37.682226 140607309653760 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "input_shape=[512, 144]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 16\n",
      "Epoch 1/15\n",
      "350/350 [==============================] - 16s 46ms/step - loss: 0.8811 - acc: 0.6824 - val_loss: 0.1503 - val_acc: 0.8649\n",
      "Epoch 2/15\n",
      "350/350 [==============================] - 15s 44ms/step - loss: 0.3746 - acc: 0.8239 - val_loss: 0.1199 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      "350/350 [==============================] - 16s 45ms/step - loss: 0.3555 - acc: 0.8327 - val_loss: 0.1034 - val_acc: 1.0000\n",
      "Epoch 4/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.3405 - acc: 0.8395 - val_loss: 0.0952 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.3371 - acc: 0.8402 - val_loss: 0.0915 - val_acc: 1.0000\n",
      "Epoch 6/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.3246 - acc: 0.8531 - val_loss: 0.0858 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.3208 - acc: 0.8551 - val_loss: 0.0837 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.3117 - acc: 0.8698 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.2702 - acc: 0.8917 - val_loss: 0.0672 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.2365 - acc: 0.9070 - val_loss: 0.0574 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.1841 - acc: 0.9212 - val_loss: 0.0531 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.1644 - acc: 0.9317 - val_loss: 0.0405 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 0.1566 - acc: 0.9382 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "350/350 [==============================] - 16s 45ms/step - loss: 0.1509 - acc: 0.9450 - val_loss: 0.0406 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "350/350 [==============================] - 16s 45ms/step - loss: 0.1599 - acc: 0.9450 - val_loss: 0.0383 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(x_images_path):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    while True:\n",
    "        for sess in x_images_path:\n",
    "            for run in x_images_path[sess]:\n",
    "                file_path= os.path.join(stimuli_features_dir, \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "                X = loadFeatureVector(file_path)\n",
    "                Y = utils.to_categorical(np.transpose(y_labels['sess01']['run02']))\n",
    "                yield (X,Y)\n",
    "            \n",
    "\n",
    "EPOCHS=15\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "Input image shape: (1, 512, 144)\n",
      "[[0.9702159  0.02873369 0.00105043]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"here\")\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and test set\n",
    "# For milestone, first we try to train a classifier network using feature vectors from above\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "#concatenate all feature vectors from all sessions/runs\n",
    "x_all_list = []\n",
    "for (currDir, _, fileList) in os.walk(stimuli_features_dir):\n",
    "    currBaseDir = os.path.basename(currDir)\n",
    "    for filename in fileList:\n",
    "        fullFilename = os.path.join(currDir, filename)\n",
    "        print(fullFilename)\n",
    "        x_content = np.load(fullFilename, allow_pickle=True).item()\n",
    "        for sess in x_content:\n",
    "            for run in x_content[sess]:\n",
    "                x_all_list.extend(x_content[sess][run])\n",
    "\n",
    "print(x_all_list[0].shape)\n",
    "print(x_all_list[1].shape)\n",
    "print(x_all_list[43].shape)\n",
    "x_all = np.asarray(x_all_list)\n",
    "print(x_all.shape)\n",
    "\n",
    "#for sess in x_images_path:\n",
    "#    for run in x_images_path[sess]:\n",
    "#        file_path = os.path.join('stimulifeatures', \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "#        x_content = np.load(file_path)\n",
    "#        print(x_content.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for sess in x_images_path:\n",
    "#    for run in x_images_path[sess]:\n",
    "#        file_path = os.path.join('stimulifeatures', \"_\".join([sCS, sess, run]) + \".npy\")\n",
    "#        x_content = np.load(file_path)\n",
    "#        print(x_content.keys())\n",
    "            \n",
    "\n",
    "#print(x_content['sess01']['run01'][0].shape)\n",
    "#print(x_content['sess01']['run01'][3].shape)\n",
    "#print(y_labels['sess01']['run01'].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create train and test test\n",
    "x_all = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# split train test set\n",
    "# del loaded data\n",
    "# assert on expected shape\n",
    "# assert x_train.shape == ()\n",
    "\n",
    "#K.set_image_date_format('channels_last')\n",
    "\n",
    "#mnist = tf.keras.datasets.mnist\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_val, x_test = np.split(x_test, 2)\n",
    "#y_val, y_test = np.split(y_test, 2)\n",
    "\n",
    "#x_train, x_val, x_test = x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "#print(x_train.shape)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train_orig), (x_test_orig, y_test_orig) = cifar10.load_data()\n",
    "x_val, x_test = np.split(x_test_orig, 2)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = utils.to_categorical(y_train_orig, num_classes)\n",
    "y_test_1 = utils.to_categorical(y_test_orig, num_classes)\n",
    "\n",
    "#y_val, y_test = np.split(y_test_1, 2)\n",
    "del x_test_orig\n",
    "del y_test_orig\n",
    "del y_train_orig\n",
    "del y_test_1\n",
    "\n",
    "#x_train, x_val, x_test = x_train / 255.0, x_val/255.0, x_train/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(num_classes),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def classifer_cnn(input, name):\n",
    "#    X = \n",
    "#    X = Conv2D()\n",
    "#    return Model(input, X, name=name)\n",
    "#input_shape =(28, 28, 3)\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "#                           activation='relu',\n",
    "#                           input_shape=input_shape),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(10, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "#                 activation='relu',\n",
    "#                 input_shape=input_shape))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "!mkdir -p models\n",
    "#%notebook -e models/{VERSION}.ipynb\n",
    "\n",
    "#K.clear_session()\n",
    "#K.set_session(tf.Session(config=))\n",
    "#model = classifer_cnn(x_in, 'classifer_cnn')\n",
    "#model.save(f'models/{VERSION}.architecture.h5')\n",
    "#model.save('models/latest.architecture.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=40\n",
    "#K.set_image_date_format('channels_last')\n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch < 10:\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "#callbacks\n",
    "#tensorboard = Tensorboard(log_di='logs/' + VERSION)\n",
    "#checkpoint_file_path = f'models/{VERSION}/{VERSION}.checkpoint.h5'\n",
    "checkpoint_file_path = 'weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_file_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2),\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                              patience=5, min_lr=0.001)\n",
    "#learning_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "#callbacks = [model_checkpoint, reduce_lr]\n",
    "#callbacks = [model_checkpoint, reduce_lr, learning_scheduler]\n",
    "#callbacks = [early_stopping, model_checkpoint]\n",
    "#callbacks = [early_stopping]\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=256, validation_data=(x_val, y_val))\n",
    "train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=256, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "#plotter.losses(history)\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n",
    "#initial_epoch += EPOCHS\n",
    "#model.save( f'models/{VERSION}.checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plotter.losses(history)\n",
    "initial_epoch += 13\n",
    "model.save( f'models/{VERSION}.checkpoint.h5')\n",
    "\n",
    "#initial_epoch += EPOCHS\n",
    "#model.save( f'models/{VERSION}/{VERSION}.checkpoint.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "my_image = imread(img_path)\n",
    "imshow(my_image)\n",
    "\n",
    "img = image.load_img(img_path, target_size=(32, 32))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sample = x_train[120]\n",
    "y_sample = y_train[120]\n",
    "print(x_sample.shape)\n",
    "imshow(x_sample)\n",
    "print(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
