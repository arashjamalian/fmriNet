{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulusDirPath: images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 14,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 10\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "        \"start_sess\": 14,\n",
    "        \"last_sess\": 15,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 10\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "#print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "#print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "#print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "print('done')\n",
    "print(\"end time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 22:49:24.709254 140057303791360 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1104 22:44:25.661679 140057303791360 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(64, activation='tanh')(X)\n",
    "    #X = Dense(16, activation='tanh')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X_input)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 12987\n",
      "Total number of dev examples: 999\n",
      "steps_per_epoch: 351\n",
      "Epoch 1/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.8731 - categorical_accuracy: 0.6200\n",
      "Epoch 00001: saving model to weights.01.h5\n",
      "351/351 [==============================] - 5s 13ms/step - loss: 0.8707 - categorical_accuracy: 0.6206 - val_loss: 0.6004 - val_categorical_accuracy: 0.7477\n",
      "Epoch 2/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.6173 - categorical_accuracy: 0.7357\n",
      "Epoch 00002: saving model to weights.02.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.6166 - categorical_accuracy: 0.7357 - val_loss: 0.5425 - val_categorical_accuracy: 0.7808\n",
      "Epoch 3/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.5252 - categorical_accuracy: 0.7833\n",
      "Epoch 00003: saving model to weights.03.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.5240 - categorical_accuracy: 0.7839 - val_loss: 0.4920 - val_categorical_accuracy: 0.8038\n",
      "Epoch 4/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.4444 - categorical_accuracy: 0.8214\n",
      "Epoch 00004: saving model to weights.04.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.4436 - categorical_accuracy: 0.8215 - val_loss: 0.4552 - val_categorical_accuracy: 0.8238\n",
      "Epoch 5/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.3986 - categorical_accuracy: 0.8430\n",
      "Epoch 00005: saving model to weights.05.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.3988 - categorical_accuracy: 0.8433 - val_loss: 0.4077 - val_categorical_accuracy: 0.8408\n",
      "Epoch 6/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.3711 - categorical_accuracy: 0.8555\n",
      "Epoch 00006: saving model to weights.06.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.3708 - categorical_accuracy: 0.8556 - val_loss: 0.3944 - val_categorical_accuracy: 0.8488\n",
      "Epoch 7/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.3381 - categorical_accuracy: 0.8709\n",
      "Epoch 00007: saving model to weights.07.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.3381 - categorical_accuracy: 0.8708 - val_loss: 0.3541 - val_categorical_accuracy: 0.8639\n",
      "Epoch 8/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.2888 - categorical_accuracy: 0.8912\n",
      "Epoch 00008: saving model to weights.08.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2890 - categorical_accuracy: 0.8912 - val_loss: 0.3420 - val_categorical_accuracy: 0.8749\n",
      "Epoch 9/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.2843 - categorical_accuracy: 0.8907\n",
      "Epoch 00009: saving model to weights.09.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2844 - categorical_accuracy: 0.8906 - val_loss: 0.3302 - val_categorical_accuracy: 0.8909\n",
      "Epoch 10/50\n",
      "345/351 [============================>.] - ETA: 0s - loss: 0.2647 - categorical_accuracy: 0.9024\n",
      "Epoch 00010: saving model to weights.10.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2644 - categorical_accuracy: 0.9027 - val_loss: 0.2967 - val_categorical_accuracy: 0.8979\n",
      "Epoch 11/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.2392 - categorical_accuracy: 0.9123\n",
      "Epoch 00011: saving model to weights.11.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2388 - categorical_accuracy: 0.9124 - val_loss: 0.2807 - val_categorical_accuracy: 0.8989\n",
      "Epoch 12/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.2224 - categorical_accuracy: 0.9180\n",
      "Epoch 00012: saving model to weights.12.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2220 - categorical_accuracy: 0.9181 - val_loss: 0.2682 - val_categorical_accuracy: 0.9049\n",
      "Epoch 13/50\n",
      "345/351 [============================>.] - ETA: 0s - loss: 0.2106 - categorical_accuracy: 0.9246\n",
      "Epoch 00013: saving model to weights.13.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.2101 - categorical_accuracy: 0.9251 - val_loss: 0.2659 - val_categorical_accuracy: 0.9029\n",
      "Epoch 14/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.1945 - categorical_accuracy: 0.9309\n",
      "Epoch 00014: saving model to weights.14.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1943 - categorical_accuracy: 0.9311 - val_loss: 0.2309 - val_categorical_accuracy: 0.9119\n",
      "Epoch 15/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1815 - categorical_accuracy: 0.9328\n",
      "Epoch 00015: saving model to weights.15.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1814 - categorical_accuracy: 0.9328 - val_loss: 0.2675 - val_categorical_accuracy: 0.9059\n",
      "Epoch 16/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.1666 - categorical_accuracy: 0.9419\n",
      "Epoch 00016: saving model to weights.16.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1662 - categorical_accuracy: 0.9421 - val_loss: 0.2309 - val_categorical_accuracy: 0.9099\n",
      "Epoch 17/50\n",
      "345/351 [============================>.] - ETA: 0s - loss: 0.1614 - categorical_accuracy: 0.9422\n",
      "Epoch 00017: saving model to weights.17.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1613 - categorical_accuracy: 0.9422 - val_loss: 0.2348 - val_categorical_accuracy: 0.9139\n",
      "Epoch 18/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9430\n",
      "Epoch 00018: saving model to weights.18.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1571 - categorical_accuracy: 0.9430 - val_loss: 0.2134 - val_categorical_accuracy: 0.9179\n",
      "Epoch 19/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.1355 - categorical_accuracy: 0.9511\n",
      "Epoch 00019: saving model to weights.19.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1354 - categorical_accuracy: 0.9513 - val_loss: 0.2156 - val_categorical_accuracy: 0.9219\n",
      "Epoch 20/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.1358 - categorical_accuracy: 0.9551\n",
      "Epoch 00020: saving model to weights.20.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.1354 - categorical_accuracy: 0.9551 - val_loss: 0.2320 - val_categorical_accuracy: 0.9219\n",
      "Epoch 21/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.1383 - categorical_accuracy: 0.9505\n",
      "Epoch 00021: saving model to weights.21.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1385 - categorical_accuracy: 0.9503 - val_loss: 0.2295 - val_categorical_accuracy: 0.9179\n",
      "Epoch 22/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.1290 - categorical_accuracy: 0.9550\n",
      "Epoch 00022: saving model to weights.22.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1286 - categorical_accuracy: 0.9553 - val_loss: 0.1871 - val_categorical_accuracy: 0.9309\n",
      "Epoch 23/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1240 - categorical_accuracy: 0.9576\n",
      "Epoch 00023: saving model to weights.23.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1239 - categorical_accuracy: 0.9576 - val_loss: 0.2116 - val_categorical_accuracy: 0.9249\n",
      "Epoch 24/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.1326 - categorical_accuracy: 0.9516\n",
      "Epoch 00024: saving model to weights.24.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1322 - categorical_accuracy: 0.9517 - val_loss: 0.1888 - val_categorical_accuracy: 0.9289\n",
      "Epoch 25/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.1127 - categorical_accuracy: 0.9615\n",
      "Epoch 00025: saving model to weights.25.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1127 - categorical_accuracy: 0.9615 - val_loss: 0.1538 - val_categorical_accuracy: 0.9419\n",
      "Epoch 26/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.1077 - categorical_accuracy: 0.9622\n",
      "Epoch 00026: saving model to weights.26.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1075 - categorical_accuracy: 0.9623 - val_loss: 0.1572 - val_categorical_accuracy: 0.9409\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/351 [============================>.] - ETA: 0s - loss: 0.0989 - categorical_accuracy: 0.9661\n",
      "Epoch 00027: saving model to weights.27.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0993 - categorical_accuracy: 0.9660 - val_loss: 0.1812 - val_categorical_accuracy: 0.9499\n",
      "Epoch 28/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.0998 - categorical_accuracy: 0.9650\n",
      "Epoch 00028: saving model to weights.28.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0999 - categorical_accuracy: 0.9650 - val_loss: 0.1556 - val_categorical_accuracy: 0.9349\n",
      "Epoch 29/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0995 - categorical_accuracy: 0.9646\n",
      "Epoch 00029: saving model to weights.29.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.0998 - categorical_accuracy: 0.9646 - val_loss: 0.1473 - val_categorical_accuracy: 0.9479\n",
      "Epoch 30/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.1133 - categorical_accuracy: 0.9607\n",
      "Epoch 00030: saving model to weights.30.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1129 - categorical_accuracy: 0.9609 - val_loss: 0.1391 - val_categorical_accuracy: 0.9560\n",
      "Epoch 31/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.1004 - categorical_accuracy: 0.9642\n",
      "Epoch 00031: saving model to weights.31.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.1005 - categorical_accuracy: 0.9643 - val_loss: 0.1363 - val_categorical_accuracy: 0.9520\n",
      "Epoch 32/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0853 - categorical_accuracy: 0.9686\n",
      "Epoch 00032: saving model to weights.32.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0855 - categorical_accuracy: 0.9686 - val_loss: 0.1556 - val_categorical_accuracy: 0.9530\n",
      "Epoch 33/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.0854 - categorical_accuracy: 0.9700\n",
      "Epoch 00033: saving model to weights.33.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.0853 - categorical_accuracy: 0.9700 - val_loss: 0.1264 - val_categorical_accuracy: 0.9530\n",
      "Epoch 34/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.0765 - categorical_accuracy: 0.9733\n",
      "Epoch 00034: saving model to weights.34.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0766 - categorical_accuracy: 0.9732 - val_loss: 0.1573 - val_categorical_accuracy: 0.9540\n",
      "Epoch 35/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0727 - categorical_accuracy: 0.9731\n",
      "Epoch 00035: saving model to weights.35.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.0725 - categorical_accuracy: 0.9731 - val_loss: 0.1289 - val_categorical_accuracy: 0.9600\n",
      "Epoch 36/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0791 - categorical_accuracy: 0.9722\n",
      "Epoch 00036: saving model to weights.36.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.0785 - categorical_accuracy: 0.9724 - val_loss: 0.1354 - val_categorical_accuracy: 0.9590\n",
      "Epoch 37/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.0788 - categorical_accuracy: 0.9722\n",
      "Epoch 00037: saving model to weights.37.h5\n",
      "351/351 [==============================] - 4s 11ms/step - loss: 0.0790 - categorical_accuracy: 0.9721 - val_loss: 0.1253 - val_categorical_accuracy: 0.9610\n",
      "Epoch 38/50\n",
      "350/351 [============================>.] - ETA: 0s - loss: 0.0823 - categorical_accuracy: 0.9705\n",
      "Epoch 00038: saving model to weights.38.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0823 - categorical_accuracy: 0.9705 - val_loss: 0.1367 - val_categorical_accuracy: 0.9570\n",
      "Epoch 39/50\n",
      "348/351 [============================>.] - ETA: 0s - loss: 0.0638 - categorical_accuracy: 0.9777\n",
      "Epoch 00039: saving model to weights.39.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0637 - categorical_accuracy: 0.9777 - val_loss: 0.0996 - val_categorical_accuracy: 0.9680\n",
      "Epoch 40/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0627 - categorical_accuracy: 0.9775\n",
      "Epoch 00040: saving model to weights.40.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0628 - categorical_accuracy: 0.9774 - val_loss: 0.1199 - val_categorical_accuracy: 0.9700\n",
      "Epoch 41/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.0590 - categorical_accuracy: 0.9785\n",
      "Epoch 00041: saving model to weights.41.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0587 - categorical_accuracy: 0.9785 - val_loss: 0.1084 - val_categorical_accuracy: 0.9690\n",
      "Epoch 42/50\n",
      "345/351 [============================>.] - ETA: 0s - loss: 0.0551 - categorical_accuracy: 0.9810\n",
      "Epoch 00042: saving model to weights.42.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0546 - categorical_accuracy: 0.9812 - val_loss: 0.1117 - val_categorical_accuracy: 0.9640\n",
      "Epoch 43/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0657 - categorical_accuracy: 0.9768\n",
      "Epoch 00043: saving model to weights.43.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0666 - categorical_accuracy: 0.9766 - val_loss: 0.0929 - val_categorical_accuracy: 0.9620\n",
      "Epoch 44/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0582 - categorical_accuracy: 0.9802\n",
      "Epoch 00044: saving model to weights.44.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0578 - categorical_accuracy: 0.9804 - val_loss: 0.1180 - val_categorical_accuracy: 0.9620\n",
      "Epoch 45/50\n",
      "346/351 [============================>.] - ETA: 0s - loss: 0.0571 - categorical_accuracy: 0.9803\n",
      "Epoch 00045: saving model to weights.45.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0574 - categorical_accuracy: 0.9801 - val_loss: 0.1080 - val_categorical_accuracy: 0.9600\n",
      "Epoch 46/50\n",
      "345/351 [============================>.] - ETA: 0s - loss: 0.0494 - categorical_accuracy: 0.9826\n",
      "Epoch 00046: saving model to weights.46.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0492 - categorical_accuracy: 0.9826 - val_loss: 0.0853 - val_categorical_accuracy: 0.9770\n",
      "Epoch 47/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.0493 - categorical_accuracy: 0.9826\n",
      "Epoch 00047: saving model to weights.47.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0492 - categorical_accuracy: 0.9827 - val_loss: 0.1086 - val_categorical_accuracy: 0.9670\n",
      "Epoch 48/50\n",
      "349/351 [============================>.] - ETA: 0s - loss: 0.0521 - categorical_accuracy: 0.9814\n",
      "Epoch 00048: saving model to weights.48.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0521 - categorical_accuracy: 0.9814 - val_loss: 0.1013 - val_categorical_accuracy: 0.9650\n",
      "Epoch 49/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.0564 - categorical_accuracy: 0.9812\n",
      "Epoch 00049: saving model to weights.49.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0560 - categorical_accuracy: 0.9814 - val_loss: 0.0869 - val_categorical_accuracy: 0.9660\n",
      "Epoch 50/50\n",
      "347/351 [============================>.] - ETA: 0s - loss: 0.0719 - categorical_accuracy: 0.9741\n",
      "Epoch 00050: saving model to weights.50.h5\n",
      "351/351 [==============================] - 4s 10ms/step - loss: 0.0717 - categorical_accuracy: 0.9741 - val_loss: 0.0932 - val_categorical_accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(data_split, data_type):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = data_split.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=50\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=4),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "#callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=(x_dev, y_dev))\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (1, 512, 144)\n",
      "[[9.9982518e-01 1.6429946e-04 1.0603078e-05]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(512, activation='tanh')(X)\n",
    "    X = Dense(128, activation='tanh')(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "input_shape = x_content.shape[1:]\n",
    "model2 = dnn_classifier(input_shape, num_classes)\n",
    "\n",
    "EPOCHS=100\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "#                                    validation_steps=validation_steps) \n",
    "train_history = model2.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
