{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stimulusDirPath: images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 15,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 10\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI4\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 9,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 10\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "#print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "#print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "#print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2019-11-04T05:33:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1104 05:33:40.565962 140195940103936 deprecation_wrapper.py:119] From /home/ubuntu/fmriNet/fmriNet/nst_utils.py:127: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: stimulifeatures/CSI4_sess01_run09.npy\n",
      "participant: CSI4, sess: sess01, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess02_run09.npy\n",
      "participant: CSI4, sess: sess02, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess03_run09.npy\n",
      "participant: CSI4, sess: sess03, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess04_run09.npy\n",
      "participant: CSI4, sess: sess04, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess05_run09.npy\n",
      "participant: CSI4, sess: sess05, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess06_run09.npy\n",
      "participant: CSI4, sess: sess06, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess07_run09.npy\n",
      "participant: CSI4, sess: sess07, run: run09\n",
      "file_path: stimulifeatures/CSI4_sess08_run09.npy\n",
      "participant: CSI4, sess: sess08, run: run09\n",
      "done\n",
      "end time: 2019-11-04T05:34:15\n"
     ]
    }
   ],
   "source": [
    "print(\"start time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)\n",
    "\n",
    "print('done')\n",
    "print(\"end time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 05:34:46.188024 140195940103936 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 512, 144)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "model = dnn_classifier(input_shape, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 13986\n",
      "Total number of dev examples: 2664\n",
      "steps_per_epoch: 378\n",
      "Epoch 1/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9465\n",
      "Epoch 00001: saving model to weights.01.h5\n",
      "378/378 [==============================] - 5s 14ms/step - loss: 0.1281 - acc: 0.9464 - val_loss: 0.0265 - val_acc: 0.9884\n",
      "Epoch 2/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9521\n",
      "Epoch 00002: saving model to weights.02.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.1231 - acc: 0.9525 - val_loss: 0.0243 - val_acc: 0.9902\n",
      "Epoch 3/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9584\n",
      "Epoch 00003: saving model to weights.03.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.1022 - acc: 0.9583 - val_loss: 0.0187 - val_acc: 0.9929\n",
      "Epoch 4/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9625\n",
      "Epoch 00004: saving model to weights.04.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0891 - acc: 0.9625 - val_loss: 0.0124 - val_acc: 0.9932\n",
      "Epoch 5/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9681\n",
      "Epoch 00005: saving model to weights.05.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0752 - acc: 0.9683 - val_loss: 0.0086 - val_acc: 0.9962\n",
      "Epoch 6/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9670\n",
      "Epoch 00006: saving model to weights.06.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0734 - acc: 0.9670 - val_loss: 0.0132 - val_acc: 0.9944\n",
      "Epoch 7/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9720\n",
      "Epoch 00007: saving model to weights.07.h5\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0637 - acc: 0.9720 - val_loss: 0.0115 - val_acc: 0.9932\n",
      "Epoch 8/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9702\n",
      "Epoch 00008: saving model to weights.08.h5\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0659 - acc: 0.9700 - val_loss: 0.0088 - val_acc: 0.9955\n",
      "Epoch 9/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9743\n",
      "Epoch 00009: saving model to weights.09.h5\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0625 - acc: 0.9743 - val_loss: 0.0071 - val_acc: 0.9974\n",
      "Epoch 10/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9720\n",
      "Epoch 00010: saving model to weights.10.h5\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0598 - acc: 0.9720 - val_loss: 0.0078 - val_acc: 0.9959\n",
      "Epoch 11/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9749\n",
      "Epoch 00011: saving model to weights.11.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0540 - acc: 0.9748 - val_loss: 0.0082 - val_acc: 0.9977\n",
      "Epoch 12/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9741\n",
      "Epoch 00012: saving model to weights.12.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0573 - acc: 0.9741 - val_loss: 0.0072 - val_acc: 0.9966\n",
      "Epoch 13/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9760\n",
      "Epoch 00013: saving model to weights.13.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0532 - acc: 0.9760 - val_loss: 0.0071 - val_acc: 0.9962\n",
      "Epoch 14/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9765\n",
      "Epoch 00014: saving model to weights.14.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0539 - acc: 0.9765 - val_loss: 0.0045 - val_acc: 0.9981\n",
      "Epoch 15/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9770\n",
      "Epoch 00015: saving model to weights.15.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0499 - acc: 0.9769 - val_loss: 0.0101 - val_acc: 0.9989\n",
      "Epoch 16/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9782\n",
      "Epoch 00016: saving model to weights.16.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0527 - acc: 0.9783 - val_loss: 0.0202 - val_acc: 0.9959\n",
      "Epoch 17/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9765\n",
      "Epoch 00017: saving model to weights.17.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0548 - acc: 0.9766 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 18/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9770\n",
      "Epoch 00018: saving model to weights.18.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0546 - acc: 0.9770 - val_loss: 0.0153 - val_acc: 0.9925\n",
      "Epoch 19/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9789\n",
      "Epoch 00019: saving model to weights.19.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0475 - acc: 0.9790 - val_loss: 0.0076 - val_acc: 0.9970\n",
      "Epoch 20/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9807\n",
      "Epoch 00020: saving model to weights.20.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0489 - acc: 0.9807 - val_loss: 0.0074 - val_acc: 0.9970\n",
      "Epoch 21/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9823\n",
      "Epoch 00021: saving model to weights.21.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0395 - acc: 0.9823 - val_loss: 0.0054 - val_acc: 0.9974\n",
      "Epoch 22/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9805\n",
      "Epoch 00022: saving model to weights.22.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0505 - acc: 0.9806 - val_loss: 0.0058 - val_acc: 0.9989\n",
      "Epoch 23/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9824\n",
      "Epoch 00023: saving model to weights.23.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0413 - acc: 0.9825 - val_loss: 0.0050 - val_acc: 0.9985\n",
      "Epoch 24/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9810\n",
      "Epoch 00024: saving model to weights.24.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0458 - acc: 0.9810 - val_loss: 0.0075 - val_acc: 0.9970\n",
      "Epoch 25/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9827\n",
      "Epoch 00025: saving model to weights.25.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0402 - acc: 0.9828 - val_loss: 0.0058 - val_acc: 0.9977\n",
      "Epoch 26/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9814\n",
      "Epoch 00026: saving model to weights.26.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0425 - acc: 0.9815 - val_loss: 0.0045 - val_acc: 0.9974\n",
      "Epoch 27/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9823\n",
      "Epoch 00027: saving model to weights.27.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0424 - acc: 0.9824 - val_loss: 0.0093 - val_acc: 0.9959\n",
      "Epoch 28/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9793\n",
      "Epoch 00028: saving model to weights.28.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0491 - acc: 0.9795 - val_loss: 0.0035 - val_acc: 0.9989\n",
      "Epoch 29/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9835\n",
      "Epoch 00029: saving model to weights.29.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0448 - acc: 0.9836 - val_loss: 0.0075 - val_acc: 0.9977\n",
      "Epoch 30/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9823\n",
      "Epoch 00030: saving model to weights.30.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0392 - acc: 0.9824 - val_loss: 0.0070 - val_acc: 0.9970\n",
      "Epoch 31/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9847\n",
      "Epoch 00031: saving model to weights.31.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0373 - acc: 0.9847 - val_loss: 0.0042 - val_acc: 0.9989\n",
      "Epoch 32/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9839\n",
      "Epoch 00032: saving model to weights.32.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0371 - acc: 0.9840 - val_loss: 0.0066 - val_acc: 0.9974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9882\n",
      "Epoch 00033: saving model to weights.33.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0303 - acc: 0.9882 - val_loss: 0.0049 - val_acc: 0.9985\n",
      "Epoch 34/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9845\n",
      "Epoch 00034: saving model to weights.34.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0426 - acc: 0.9842 - val_loss: 0.0084 - val_acc: 0.9974\n",
      "Epoch 35/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9872\n",
      "Epoch 00035: saving model to weights.35.h5\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0337 - acc: 0.9873 - val_loss: 0.0033 - val_acc: 0.9989\n",
      "Epoch 36/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9838\n",
      "Epoch 00036: saving model to weights.36.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0444 - acc: 0.9839 - val_loss: 0.0029 - val_acc: 0.9996\n",
      "Epoch 37/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9880\n",
      "Epoch 00037: saving model to weights.37.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0303 - acc: 0.9881 - val_loss: 0.0028 - val_acc: 0.9996\n",
      "Epoch 38/50\n",
      "374/378 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9874\n",
      "Epoch 00038: saving model to weights.38.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0326 - acc: 0.9874 - val_loss: 0.0164 - val_acc: 0.9974\n",
      "Epoch 39/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9877\n",
      "Epoch 00039: saving model to weights.39.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0382 - acc: 0.9877 - val_loss: 0.0047 - val_acc: 0.9985\n",
      "Epoch 40/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9871\n",
      "Epoch 00040: saving model to weights.40.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0344 - acc: 0.9871 - val_loss: 0.0052 - val_acc: 0.9989\n",
      "Epoch 41/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9880\n",
      "Epoch 00041: saving model to weights.41.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0324 - acc: 0.9881 - val_loss: 0.0029 - val_acc: 0.9996\n",
      "Epoch 42/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9885\n",
      "Epoch 00042: saving model to weights.42.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0338 - acc: 0.9884 - val_loss: 0.0052 - val_acc: 0.9989\n",
      "Epoch 43/50\n",
      "377/378 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9880\n",
      "Epoch 00043: saving model to weights.43.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0331 - acc: 0.9880 - val_loss: 0.0037 - val_acc: 0.9989\n",
      "Epoch 44/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9891\n",
      "Epoch 00044: saving model to weights.44.h5\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0304 - acc: 0.9891 - val_loss: 0.0047 - val_acc: 0.9989\n",
      "Epoch 45/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9898\n",
      "Epoch 00045: saving model to weights.45.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0314 - acc: 0.9899 - val_loss: 0.0039 - val_acc: 0.9989\n",
      "Epoch 46/50\n",
      "373/378 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9883\n",
      "Epoch 00046: saving model to weights.46.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0351 - acc: 0.9882 - val_loss: 0.0026 - val_acc: 0.9992\n",
      "Epoch 47/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9891\n",
      "Epoch 00047: saving model to weights.47.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0307 - acc: 0.9892 - val_loss: 0.0039 - val_acc: 0.9992\n",
      "Epoch 48/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9883\n",
      "Epoch 00048: saving model to weights.48.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0307 - acc: 0.9883 - val_loss: 0.0044 - val_acc: 0.9985\n",
      "Epoch 49/50\n",
      "375/378 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9882\n",
      "Epoch 00049: saving model to weights.49.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0354 - acc: 0.9882 - val_loss: 0.0041 - val_acc: 0.9996\n",
      "Epoch 50/50\n",
      "376/378 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9915\n",
      "Epoch 00050: saving model to weights.50.h5\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0263 - acc: 0.9916 - val_loss: 0.0037 - val_acc: 0.9989\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(x_images_path, data_type):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = x_images_path.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=50\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"train\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (1, 512, 144)\n",
      "[[1.0000000e+00 1.5653412e-17 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
