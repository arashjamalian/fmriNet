{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50, VGG19\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "#from keras.preprocessing import image\n",
    "#from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import nibabel as nib\n",
    "import re\n",
    "from collections import Counter\n",
    "#import imageio\n",
    "from nst_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "#%aimport \n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#tf.set_random_seed(SEED)\n",
    "tf.random.set_seed\n",
    "\n",
    "K.clear_session()\n",
    "#K.set_image_data_format('channels_last')\n",
    "#K.set_learning_phase(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For phase1, training examples are images shown to 4 participants across multiple sessions.\n",
    "\n",
    "Images labeled for 3 classes: scenes, coco, imgnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a global list of images file path with index matching trained data\n",
    "\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "\n",
    "global_data = {\n",
    "        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\", \"CSI4\"],\n",
    "        \"start_sess\": 0,\n",
    "        \"last_sess\": 16,\n",
    "        \"start_run\": 0,\n",
    "        \"last_run\": 15\n",
    "}\n",
    "\n",
    "gList = {}\n",
    "global_index = 0\n",
    "imagePathList = []\n",
    "blankImage = 'Blank'\n",
    "\n",
    "for participant in global_data['participant_list']:\n",
    "        print()\n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        gList[participant] = {}\n",
    "        for sNum in range(global_data['start_sess'], global_data['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            gList[participant][sSes] = {}\n",
    "            for rNum in range(global_data['start_run'], global_data['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                if not os.path.exists(dir_path):\n",
    "                    continue\n",
    "\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                if not os.path.exists(stimulusListFilename):\n",
    "                    continue\n",
    "                    \n",
    "                print(\"cs: %s sess: %s run: %s\" % (participant, sNum, rNum))\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    imageList = f.read().splitlines()\n",
    "                    gList[participant][sSes][sRun] = imageList\n",
    "                    #global_index += len(fileList)\n",
    "                    for imageFileName in imageList:\n",
    "                        for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                            currBaseDir = os.path.basename(currDir)\n",
    "                            for filename in fileList:\n",
    "                                if filename in imageFileName:\n",
    "                                    fullFilename = os.path.join(currDir, filename)\n",
    "                                    imagePathList.append(fullFilename)\n",
    "                                    #print(fullFilename)\n",
    "                                    break\n",
    "\n",
    "                #last index for no image\n",
    "                #global_index += 1\n",
    "                imagePathList.append(blankImage)\n",
    "\n",
    "                    \n",
    "#print(global_index)\n",
    "print(len(imagePathList))\n",
    "\n",
    "# Other way to get global index from events file. just ran this to verify above\n",
    "#from glob import glob\n",
    "#import pandas as pd\n",
    "#import os\n",
    "#\n",
    "#events_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "#iCount = 0\n",
    "#imageList = []\n",
    "#for subname in ['sub-CSI1', 'sub-CSI2', 'sub-CSI3', 'sub-CSI4']:\n",
    "#    for sNum in range(0, 20):\n",
    "#        ses = \"ses-%s\" % str(sNum).zfill(2)\n",
    "#        event_path = os.path.join(events_dir,subname,ses,'func','*run*' + '_events.tsv')\n",
    "#        event = glob(event_path)\n",
    "#        if not event:\n",
    "#            continue\n",
    "#\n",
    "#        event_file = glob(os.path.join(events_dir,subname,ses,'func','*' + 'run*' + '_events.tsv'))\n",
    "#        for ev in event_file:\n",
    "#            events = pd.read_csv(ev, sep = '\\t')\n",
    "#            for index, row in events.iterrows():\n",
    "#                iCount += 1\n",
    "#                imageList.append(row['ImgName'])\n",
    "#                #print(iCount)\n",
    "#\n",
    "#            # index for no image\n",
    "#            iCount += 1\n",
    "#            imageList.append('None')\n",
    "#\n",
    "#print(iCount)\n",
    "#print(len(imageList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = Model(inputs = base_model.input, outputs = base_model.get_layer('avg_pool').output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_activations = []\n",
    "x_images = []\n",
    "for imgFile in imagePathList:\n",
    "    if imgFile is blankImage:\n",
    "        x_images.append(blankImage)\n",
    "        continue\n",
    "\n",
    "    img = image.load_img(imgFile, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x_images.append(preprocess_input(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blank_array = np.zeros((1, 2048))\n",
    "image_activations = [model.predict(x) if x is not blankImage else blank_array for x in x_images]\n",
    "print(len(image_activations))\n",
    "print(image_activations[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_all = np.concatenate(image_activations)\n",
    "print(activations_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageActivation = \"imageActivation\"\n",
    "activationFile = \"stActivations.npy\"\n",
    "imageActivationFile = os.path.join(imageActivation, activationFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!mkdir -p imageActivation\n",
    "with open(imageActivationFile, 'wb') as f:\n",
    "    pickle.dump(activations_all, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 2048)\n",
      "15.81203075060473\n",
      "-0.3625183521784735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 2048)\n",
      "[-0.00252447 -0.00222958 -0.00456031 ...  0.10309614 -0.00550695\n",
      " -0.0056969 ]\n",
      "0.2813703920418878\n",
      "-0.006450906432175714\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 10, 1100)\n",
      "17442\n",
      "(19380, 2048)\n",
      "(19380, 2048)\n",
      "xtrain shape: (17442, 10, 1100)\n",
      "ytrain shape: (17442, 2048)\n",
      "xtest shape: (1938, 10, 1100)\n",
      "ytest shape: (1938, 2048)\n",
      "Train on 17442 samples, validate on 1938 samples\n",
      "Epoch 1/10\n",
      "17442/17442 [==============================] - 19s 1ms/sample - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 4.9346e-04 - val_mean_squared_error: 4.9346e-04\n",
      "Epoch 2/10\n",
      "17442/17442 [==============================] - 18s 1ms/sample - loss: 5.2597e-04 - mean_squared_error: 5.2597e-04 - val_loss: 4.2087e-04 - val_mean_squared_error: 4.2087e-04\n",
      "Epoch 3/10\n",
      "17442/17442 [==============================] - 18s 1ms/sample - loss: 4.5911e-04 - mean_squared_error: 4.5911e-04 - val_loss: 4.1154e-04 - val_mean_squared_error: 4.1154e-04\n",
      "Epoch 4/10\n",
      "17442/17442 [==============================] - 18s 1ms/sample - loss: 4.3336e-04 - mean_squared_error: 4.3336e-04 - val_loss: 4.1262e-04 - val_mean_squared_error: 4.1262e-04\n",
      "Epoch 5/10\n",
      "17442/17442 [==============================] - 17s 993us/sample - loss: 4.2229e-04 - mean_squared_error: 4.2229e-04 - val_loss: 4.0807e-04 - val_mean_squared_error: 4.0807e-04\n",
      "Epoch 6/10\n",
      "17442/17442 [==============================] - 17s 989us/sample - loss: 4.1528e-04 - mean_squared_error: 4.1528e-04 - val_loss: 3.9635e-04 - val_mean_squared_error: 3.9635e-04\n",
      "Epoch 7/10\n",
      "17442/17442 [==============================] - 17s 1ms/sample - loss: 4.0946e-04 - mean_squared_error: 4.0946e-04 - val_loss: 4.0106e-04 - val_mean_squared_error: 4.0106e-04\n",
      "Epoch 8/10\n",
      "17442/17442 [==============================] - 17s 1ms/sample - loss: 4.0205e-04 - mean_squared_error: 4.0205e-04 - val_loss: 4.0267e-04 - val_mean_squared_error: 4.0267e-04\n",
      "Epoch 9/10\n",
      "17442/17442 [==============================] - 18s 1ms/sample - loss: 3.9974e-04 - mean_squared_error: 3.9974e-04 - val_loss: 4.0831e-04 - val_mean_squared_error: 4.0831e-04\n",
      "Epoch 10/10\n",
      "17442/17442 [==============================] - 17s 989us/sample - loss: 3.9558e-04 - mean_squared_error: 3.9558e-04 - val_loss: 3.9444e-04 - val_mean_squared_error: 3.9444e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train encoder .. fmri to image activation vector\n",
    "ROI_list = [\n",
    "    'X_LHPPA.npy', #(19380, 5, 100)\n",
    "    'X_RHLOC.npy', #(19380, 5, 170)\n",
    "    'X_LHLOC.npy', #(19380, 5, 130)\n",
    "    'X_RHEarlyVis.npy', #(19380, 5, 220)\n",
    "    'X_RHRSC.npy', #(19380, 5, 100)\n",
    "    'X_RHOPA.npy', #(19380, 5, 80)\n",
    "    'X_RHPPA.npy', #(19380, 5, 140)\n",
    "    'X_LHEarlyVis.npy', #(19380, 5, 190)\n",
    "    'X_LHRSC.npy', #(19380, 5, 30)\n",
    "    'X_LHOPA.npy', #(19380, 5, 70)\n",
    "]\n",
    "\n",
    "train_folder = '/home/ubuntu/cs230Project/dataset/traindata'\n",
    "ax_length = 5 * 220\n",
    "array_list = []\n",
    "for roc_file in ROI_list:\n",
    "    xt_file_path = os.path.join(train_folder, roc_file)\n",
    "    xtrain_n = np.load(xt_file_path)\n",
    "    # concatenate all 5 frame from each sample for now, also append all examples to 220 voxels\n",
    "    #xt = xtrain_n[:, 3, :]\n",
    "    # concatenate all 5 time frames together\n",
    "    xt = np.reshape(xtrain_n, (xtrain_n.shape[0], -1))\n",
    "    xt_pad = np.pad(xt, ((0, 0), (0, ax_length-xt.shape[1])), mode='constant', constant_values=0)\n",
    "    #print(xt_pad.shape)\n",
    "    array_list.append(xt_pad)\n",
    "\n",
    "\n",
    "x_stack = np.dstack(array_list)\n",
    "x_all = np.swapaxes(x_stack,1,2)\n",
    "#x_all = np.concatenate(array_list, axis=1)\n",
    "print(x_all.shape)\n",
    "\n",
    "num_samples = x_all.shape[0]\n",
    "#num_samples = 5000\n",
    "\n",
    "print(int(num_samples * 0.9))\n",
    "\n",
    "with open(imageActivationFile, 'rb') as f:\n",
    "    y_activations = pickle.load(f)\n",
    "\n",
    "print(y_activations.shape)\n",
    "y_normalized=[]\n",
    "for sample in range(0, y_activations.shape[0]):\n",
    "    c_array = y_activations[sample, :]\n",
    "    #c_array = c_array - np.mean(c_array) / (np.sqrt(np.var(c_array) + 1e-8))\n",
    "    c_array = c_array / (np.linalg.norm(c_array) + 1e-8)\n",
    "    y_normalized.append(c_array)\n",
    "    \n",
    "y_all = np.squeeze(np.vstack(y_normalized))\n",
    "print(y_all.shape)\n",
    "\n",
    "callbacks = [TensorBoard(log_dir=f'./log/{i}')]\n",
    "\n",
    "##split data to train and dev\n",
    "x_train = x_all[0:int(num_samples * 0.9), :, :]\n",
    "y_train = y_all[0:int(num_samples * 0.9), :]\n",
    "x_test = x_all[int(num_samples * 0.9):, :, :]\n",
    "y_test = y_all[int(num_samples * 0.9):, :]\n",
    "#x_test = x_all[int(num_samples * 0.9):num_samples, :, :]\n",
    "#y_test = y_all[int(num_samples * 0.9):num_samples, :]\n",
    "\n",
    "print(\"xtrain shape: %s\" % str(x_train.shape))\n",
    "print(\"ytrain shape: %s\" % str(y_train.shape))\n",
    "print(\"xtest shape: %s\" % str(x_test.shape))\n",
    "print(\"ytest shape: %s\" % str(y_test.shape))\n",
    "\n",
    "def auto_encoder_lstm(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = LSTM(units = 128, return_sequences = True)(X_input)\n",
    "    #LSTM(128, dropout=0.2, recurrent_dropout=0.2)\n",
    "    #X = BatchNormalization()(X_input)\n",
    "    #X = LSTM(128)(X_input)\n",
    "    X = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    #X = Dense(128, activation = \"tanh\")(X)\n",
    "    #X = BatchNormalization()(X)\n",
    "    #X = TimeDistributed(Dense(encoding_dim, activation = \"sigmoid\"))(X)\n",
    "    #X = Dense(64, activation = \"sigmoid\")(X)\n",
    "    X = Dense(encoding_dim, activation = \"tanh\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "assert x_train.shape[2] == ax_length\n",
    "assert x_train.shape[1] == len(ROI_list)\n",
    "\n",
    "\n",
    "encoder_model = auto_encoder_lstm((x_train.shape[1], x_train.shape[2]), y_train.shape[1])\n",
    "#encoder_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['cosine_proximity'])\n",
    "encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "#from keras import losses\n",
    "#encoder_model.compile(optimizer='adam', loss=losses.cosine_proximity, metrics=['cosine_proximity'])\n",
    "\n",
    "train_history = encoder_model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=50, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_file = 'encoder_model.h5'\n",
    "encoder_model.save(encoder_model_file)\n",
    "del encoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 10, 1100)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 128)               629248    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2048)              264192    \n",
      "=================================================================\n",
      "Total params: 893,952\n",
      "Trainable params: 893,696\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_model = load_model(encoder_model_file)\n",
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19380\n",
      "(1, 2048)\n",
      "(19380, 2048)\n"
     ]
    }
   ],
   "source": [
    "fmri_features = []\n",
    "for sample in range(0, x_all.shape[0]):\n",
    "    fmri_features.append(encoder_model.predict(np.expand_dims(x_all[sample, :, :], axis=0)))\n",
    "\n",
    "print(len(fmri_features))\n",
    "print(fmri_features[0].shape)\n",
    "\n",
    "activations_all = np.concatenate(fmri_features)\n",
    "print(activations_all.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19380, 2048)\n",
      "[0.00991819 0.02032384 0.01259134 ... 0.0058254  0.00718123 0.00934707]\n"
     ]
    }
   ],
   "source": [
    "print(activations_all.shape)\n",
    "fmriActivation = \"fmriActivation\"\n",
    "fmriActivationFile = os.path.join(fmriActivation, \"fmriActivations.npy\")\n",
    "!mkdir -p fmriActivation\n",
    "with open(fmriActivationFile, 'wb') as f:\n",
    "    pickle.dump(activations_all, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(imageActivationFile, 'wb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations from pretrained model and saved to file\n",
    "activations = {x: pretrained_model.predict(X_images[x]) for x in sorted(X_images.keys())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stList = {}\n",
    "stimulusDirPath = os.path.join('images', 'BOLD5000_Stimuli', 'Scene_Stimuli', 'Presented_Stimuli')\n",
    "print(\"stimulusDirPath: %s\" % stimulusDirPath)\n",
    "\n",
    "   \n",
    "#data_split = {\n",
    "#    \"train\": {\n",
    "#        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "#        \"start_sess\": 1,\n",
    "#        \"last_sess\": 14,\n",
    "#        \"start_run\": 1,\n",
    "#        \"last_run\": 10\n",
    "#    },\n",
    "#    \"dev\": {\n",
    "#        \"participant_list\": [\"CSI1\", \"CSI2\", \"CSI3\"],\n",
    "#        \"start_sess\": 14,\n",
    "#        \"last_sess\": 15,\n",
    "#        \"start_run\": 1,\n",
    "#        \"last_run\": 10\n",
    "#    }\n",
    "#}\n",
    "\n",
    "data_split = {\n",
    "    \"train\": {\n",
    "        \"participant_list\": [\"CSI1\"],\n",
    "        \"start_sess\": 1,\n",
    "        \"last_sess\": 3,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 4\n",
    "    },\n",
    "    \"dev\": {\n",
    "        \"participant_list\": [\"CSI1\"],\n",
    "        \"start_sess\": 14,\n",
    "        \"last_sess\": 15,\n",
    "        \"start_run\": 1,\n",
    "        \"last_run\": 2\n",
    "    }\n",
    "}\n",
    "classes = {'ImageNet': 0, 'COCO': 1, 'Scene': 2}\n",
    "\n",
    "# Get list of stimuli pictures shown in each session in each run\n",
    "for data_type, items in data_split.items():\n",
    "    stList[data_type] = {}\n",
    "    for participant in items['participant_list']:\n",
    "        \n",
    "        # CS1 file are missing 1 after CSI\n",
    "        if participant == \"CSI1\":\n",
    "            CSI = \"CSI\"\n",
    "        else:\n",
    "            CSI = participant\n",
    "        \n",
    "        stList[data_type][participant] = {}\n",
    "        for sNum in range(items['start_sess'], items['last_sess']):\n",
    "            sSes = \"sess\" + str(sNum).zfill(2)\n",
    "            stList[data_type][participant][sSes] = {}\n",
    "            for rNum in range(items['start_run'], items['last_run']):\n",
    "                sRun = \"run\" + str(rNum).zfill(2)\n",
    "                dir_path = os.path.join(\"images\",\"BOLD5000_Stimuli\", \"Stimuli_Presentation_Lists\",participant, participant + \"_\" + sSes)\n",
    "                #print(stimulusDirPath)\n",
    "                stimulusListFilename = os.path.join(dir_path, \"_\".join([CSI, sSes, sRun]) + \".txt\")\n",
    "                #print(stimulusListFilename)\n",
    "                with open(stimulusListFilename) as f:\n",
    "                    stList[data_type][participant][sSes][sRun] = f.read().splitlines() \n",
    "\n",
    "            \n",
    "x_images_path = {}\n",
    "y_labels = {}\n",
    "for data_type, participantDict in stList.items():\n",
    "    x_images_path[data_type] = {}\n",
    "    y_labels[data_type] = {}\n",
    "    for participant, sessDict in participantDict.items(): \n",
    "        x_images_path[data_type][participant] = {}\n",
    "        y_labels[data_type][participant] = {}\n",
    "        for sess, runDict in sessDict.items():\n",
    "            x_images_path[data_type][participant][sess] = {}\n",
    "            y_labels[data_type][participant][sess] = {}\n",
    "            for run, imageList in runDict.items():\n",
    "                x_images_path[data_type][participant][sess][run] = []\n",
    "                y_labels[data_type][participant][sess][run] = []\n",
    "                #print(\"sess: %s, run: %s\" %(sess, run))\n",
    "                labelList = []\n",
    "                for imageFileName in imageList:\n",
    "                    for (currDir, _, fileList) in os.walk(stimulusDirPath):\n",
    "                        currBaseDir = os.path.basename(currDir)\n",
    "                        for filename in fileList:\n",
    "                            if filename in imageFileName:\n",
    "                                fullFilename = os.path.join(currDir, filename)\n",
    "                                x_images_path[data_type][participant][sess][run].append(fullFilename)\n",
    "                                # using directory path to determine class\n",
    "                                labelList.append(classes.get(currDir.split('/')[-1]))\n",
    "                                break\n",
    "        \n",
    "                y_labels[data_type][participant][sess][run] = np.reshape(np.asarray(labelList), (1, -1))\n",
    "\n",
    "# Todo: normalize data\n",
    "# x_train / 255.0, x_val/255.0, x_train/255.0\n",
    "\n",
    "#print(x_images_path)\n",
    "#print(y_labels[\"train\"][\"CSI1\"]['sess01']['run01'].shape)\n",
    "#print(y_labels[\"dev\"][\"CSI3\"]['sess01']['run01'].shape)\n",
    "#print(len(x_images_path[\"train\"][\"CSI1\"]['sess01']['run02']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "Compute feature vectors using pretrained imagenet-vgg-verydeep model\n",
    "\n",
    "Feature vectors saved in file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = 'avgpool5'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "def unrollContentOutput(cOutput):\n",
    "    m, n_H, n_W, n_C = cOutput.shape\n",
    "    output = np.transpose(np.reshape(cOutput, (n_H * n_W, n_C)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "\n",
    "!mkdir -p stimulifeatures\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#sess = tf.InteractiveSession()\n",
    "#precompute content vectors from presented stimuli\n",
    "#content_layer = 'conv4_2'\n",
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    for data_type, participantDict in x_images_path.items():\n",
    "        for participant, sessDict in participantDict.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run, imageList in runDict.items():\n",
    "                    #x_content = {sess: {run: []}}\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    if os.path.exists(file_path):\n",
    "                        #print already computed, skip\n",
    "                        continue\n",
    "\n",
    "                    print(\"file_path: %s\" % file_path)\n",
    "                    print(\"participant: %s, sess: %s, run: %s\" % (participant, sess, run))\n",
    "                    contentList = []\n",
    "                    for img_path in imageList:\n",
    "                        #stImage = imread(cImage)\n",
    "                        img = image.load_img(img_path, target_size=(375, 375))\n",
    "                        x = image.img_to_array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print('Input image shape:', x.shape)\n",
    "                        #img_array = img_to_array(img)\n",
    "                        #stImage = imageio.imread(img_path)\n",
    "                        #print(\"img_path: %s\" % img_path)\n",
    "                        #print(stImage.shape)\n",
    "                        #stImage = reshape_and_normalize_image(stImage)\n",
    "                        #stImage = np.reshape(stImage, (1, 375, 375, 3))\n",
    "                        ts.run(vmodel['input'].assign(x))\n",
    "                        #a_C = sess.run(vmodel)\n",
    "                        out = vmodel[content_layer]\n",
    "                        contentOut = ts.run(out)\n",
    "                        contentList.append(unrollContentOutput(contentOut))\n",
    "            \n",
    "                    #x_content[sess][run] = np.asarray(contentList)\n",
    "                    contentArray = np.asarray(contentList)\n",
    "                    # shape is (35, 512, 144): num of pictures, channels, width*height\n",
    "                    #print(x_content[sess][run].shape)\n",
    "                    #x_content[sess][run].append(unrollContentOutput(contentOut))\n",
    "        \n",
    "                    #np.save(file_path, x_content)\n",
    "                    np.save(file_path, contentArray)\n",
    "                    #del x_content\n",
    "\n",
    "print('done')\n",
    "print(\"end time: %s\" % datetime.now().strftime('%Y-%m-%dT%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as ts:\n",
    "    vmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\n",
    "    img_path = './images/BOLD5000_Stimuli/Scene_Stimuli/Presented_Stimuli/ImageNet/n01833805_1411.JPEG'\n",
    "    img = image.load_img(img_path, target_size=(375, 375))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    ts.run(vmodel['input'].assign(x))\n",
    "    out = vmodel[content_layer]\n",
    "    predictContentOut = ts.run(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "VERSION = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "file_path = os.path.join('stimulifeatures', 'CSI2_sess01_run01.npy')\n",
    "\n",
    "x_content = np.load(file_path, allow_pickle=True)\n",
    "print(x_content.shape)\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(64, activation='tanh')(X)\n",
    "    #X = Dense(16, activation='tanh')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def dnn_gap_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = GlobalAveragePooling1D(data_format='channels_first')(X_input)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    #X = Activation('relu')(X)\n",
    "    #X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "\n",
    "def auto_encoder(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(encoding_dim, activation='relu')(X_input)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "def cnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(32, (3, 3), padding='same')(X_input)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(32, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(64, (3, 3))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(num_classes)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='cnn_classifier')\n",
    "    return model\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Flatten(input_shape=[512, 144]),\n",
    "#    tf.keras.layers.Dense(128, activation='relu'),\n",
    "#    tf.keras.layers.Dropout(0.2),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model = tf.keras.models.Sequential([\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "#                 input_shape=x_train.shape[1:]),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#    tf.keras.layers.Dropout(0.25),\n",
    "#    tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Dense(512),\n",
    "#    tf.keras.layers.Activation('relu'),\n",
    "#    tf.keras.layers.Dense(num_classes),\n",
    "#    tf.keras.layers.Activation('softmax')\n",
    "#])\n",
    "\n",
    "\n",
    "#input_shape=[512, 144]\n",
    "input_shape = x_content.shape[1:]\n",
    "print(input_shape)\n",
    "#model = dnn_classifier(input_shape, num_classes)\n",
    "model = dnn_gap_classifier(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loadFeatureVector(file_path):\n",
    "    return np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "def featureVectorLoader(data_split, data_type):\n",
    "    #every file has 35 feature vectors (one batch)   \n",
    "    x_images = data_split.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    file_path= os.path.join(stimuli_features_dir, \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    X = loadFeatureVector(file_path)\n",
    "                    Y = utils.to_categorical(np.transpose(y_labels[data_type][participant][sess][run]))\n",
    "                    yield (X,Y)\n",
    "\n",
    "EPOCHS=20\n",
    "#callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=4),\n",
    "             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "#callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "                                    validation_steps=validation_steps) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=(x_dev, y_dev))\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = load_model('weights.20.h5')\n",
    "new_model.summary()\n",
    "print(new_model.get_weights()[0].shape)\n",
    "print(new_model.get_weights()[1].shape)\n",
    "print(new_model.get_weights()[2].shape)\n",
    "print(new_model.get_weights()[3].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N=50\n",
    "arr1 = new_model.get_weights()[2][:,0]\n",
    "indices1 = np.argsort(arr1, axis=0)[-N:]\n",
    "arr2 = new_model.get_weights()[2][:,1]\n",
    "indices2 = np.argsort(arr2, axis=0)[-N:]\n",
    "arr3 = new_model.get_weights()[2][:,2]\n",
    "indices3 = np.argsort(arr3, axis=0)[-N:]\n",
    "#\n",
    "print(indices1)\n",
    "print(indices2)\n",
    "print(indices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "arr = new_model.get_weights()[0]\n",
    "N=20\n",
    "filter_select = []\n",
    "for index_list in [indices1, indices2, indices3]:\n",
    "    all_ind = []\n",
    "    for index in index_list:\n",
    "        indices = np.argsort(arr, axis=0)[-N:, index]\n",
    "        sort_ind = np.sort(indices, axis=-1)\n",
    "        all_ind.extend(list(sort_ind))\n",
    "        #print(sort_ind)\n",
    "        #plt.plot(sort_ind)\n",
    "    a_ind = [key for key,_ in Counter(all_ind).most_common()][0:10]\n",
    "    print(a_ind)\n",
    "    filter_select.extend([item for item in a_ind if item not in filter_select])\n",
    "    \n",
    "print(filter_select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nibabel\n",
    "import nibabel as nib\n",
    "import re\n",
    "fmri_data_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "fmriRegex = re.compile(r'^(.*?)_sess(.*?)_run(.*?).npy$')\n",
    "\n",
    "# At the beginning and end of each run, a fixation cross was shown for 6 sec (3TORs) and\n",
    "# 12 sec (6TORs), respectively. hence stIndex goes from 3:-6\n",
    "# 37 images shows in each run >> 185 TOR\n",
    "# Each image was presented for 1 sec followed by a 9 sec fixation cross (5TORs)\n",
    "# For each stimuls, average assocated 5 TORs and map them\n",
    "def loadFmriData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69)\n",
    "    return x\n",
    "\n",
    "def loadFmriLstmData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69) > (37, 69, 106*106)\n",
    "    x = np.swapaxes(np.reshape(x, (37, -1, 69)), 1, 2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def loadFilterVector(file_path, filterNumList):\n",
    "    all_features = np.load(file_path, allow_pickle=True)\n",
    "    features = []\n",
    "    for filterNum in filterNumList:\n",
    "        features.append(all_features[:, filterNum, :].T)\n",
    "    \n",
    "    ft = np.asarray(features)\n",
    "    return ft.reshape(-1, 37).T\n",
    "\n",
    "filterNumList = [452, 209, 327, 377, 33, 16, 433, 19, 66, 467]\n",
    "data_split = x_images_path\n",
    "data_type = \"train\"\n",
    "x_images = data_split.get(data_type, None)\n",
    "for participant, sessDict in x_images.items():\n",
    "    for sess, runDict in sessDict.items():\n",
    "        for run in runDict.keys():\n",
    "            #fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % participant, \"sess\" \"_\".join([participant, sess, run]) + \".npy\")\n",
    "            feature_file_name = \"_\".join([participant, sess, run]) + \".npy\"\n",
    "            #sub-CSI3/ses-01/func\n",
    "            # sub-CSI3_ses-09_task-5000scenes_run-05_bold.nii.gz\n",
    "            match = fmriRegex.match(feature_file_name)\n",
    "            if match:\n",
    "                  fmri_file_name = \"sub-%s_ses-%s_task-5000scenes_run-%s_bold.nii.gz\" % ( match.group(1), match.group(2), match.group(3))\n",
    "                  fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % match.group(1), \"ses-%s\" % match.group(2), \"func\", fmri_file_name)\n",
    "                  print(fmri_data_path)\n",
    "                \n",
    "            feature_vector_path= os.path.join(stimuli_features_dir, feature_file_name)\n",
    "            X = loadFmriLstmData(fmri_data_path)\n",
    "            Y = loadFilterVector(feature_vector_path, filterNumList)\n",
    "            print(X.shape)\n",
    "            print(Y.shape)\n",
    "            break\n",
    "\n",
    "                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROI_list = [\n",
    "    'X_LHPPA.npy', #(19380, 5, 100)\n",
    "    'X_RHLOC.npy', #(19380, 5, 170)\n",
    "    'X_LHLOC.npy', #(19380, 5, 130)\n",
    "    'X_RHEarlyVis.npy', #(19380, 5, 220)\n",
    "    'X_RHRSC.npy', #(19380, 5, 100)\n",
    "    'X_RHOPA.npy', #(19380, 5, 80)\n",
    "    'X_RHPPA.npy', #(19380, 5, 140)\n",
    "    'X_LHEarlyVis.npy', #(19380, 5, 190)\n",
    "    'X_LHRSC.npy', #(19380, 5, 30)\n",
    "    'X_LHOPA.npy', #(19380, 5, 70)\n",
    "]\n",
    "\n",
    "#x_all (19380, 5, 1230)\n",
    "#y_all shape (19380, 17)\n",
    "\n",
    "#one way is to concatenate last dimenesion and just use 5 time series\n",
    "# so lstm input would be \n",
    "# other way is to train each roi seperately to encode to feature vector. and then inout feature vectoers to classify\n",
    "# or input roi as LSTM nodes to get one feature vector\n",
    "\n",
    "train_folder = '/home/ubuntu/cs230Project/dataset/traindata'\n",
    "array_list = []\n",
    "for roc_file in ROI_list:\n",
    "    xt_file_path = os.path.join(train_folder, roc_file)\n",
    "    xtrain_n = np.load(xt_file_path)\n",
    "    array_list.append(xtrain_n)\n",
    "    #print(xtrain_n.shape)\n",
    "\n",
    "#all_x = np.asarray(array_list)\n",
    "#print(all_x.shape)\n",
    "x_all = np.dstack(array_list)\n",
    "print(x_all.shape)\n",
    "\n",
    "yt_file_path = os.path.join(train_folder, 'Yreal_all.npy')\n",
    "y_all = np.load(yt_file_path)\n",
    "\n",
    "num_classes = y_all.shape[1]\n",
    "\n",
    "x_train = x_all[0:18380, :, :]\n",
    "y_train = y_all[0:18380, :]\n",
    "x_test = x_all[18380:, :, :]\n",
    "y_test = y_all[18380:, :]\n",
    "\n",
    "def classifer_lstm(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = LSTM(512, dropout=0.2)(X_input)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(num_classes, activation = \"softmax\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "Tx = x_train.shape[1]\n",
    "Voxels = x_train.shape[2]\n",
    "classifier_lstm = classifer_lstm((Tx, Voxels), num_classes)\n",
    "\n",
    "EPOCHS=100\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "#callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "classifier_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = classifier_lstm.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "train_history = classifier_lstm.fit(x=x_train, y=y_train, epochs=EPOCHS, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_list[0].shape)\n",
    "print(array_list[1].shape)\n",
    "test = np.dstack((array_list[0],array_list[1]))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single layer neural network (one network per filter) to map fmri data to above filters\n",
    "# input: X of shape (106, 106, 69, 194) \n",
    "# output: Y of shape (144, 1) image features on specific filters\n",
    "fmriRegex = re.compile(r'^(.*?)_sess(.*?)_run(.*?).npy$')\n",
    "\n",
    "fmri_data_dir = '/home/ubuntu/cs230Project/dataset/ds001499-download'\n",
    "stimuli_features_dir = 'stimulifeatures'\n",
    "\n",
    "def loadFilterVector(file_path, filterNumList):\n",
    "    all_features = np.load(file_path, allow_pickle=True)\n",
    "    #features = []\n",
    "    #for filterNum in filterNumList:\n",
    "    #    features.append(all_features[:, filterNum, :].T)\n",
    "    \n",
    "    #ft = np.asarray(features)\n",
    "    #return ft.reshape(-1, 37).T\n",
    "    #return all_features[:, 377, :]\n",
    "    feat_sel = all_features[:, filterNumList, :]\n",
    "    sel_shape = feat_sel.shape[0]\n",
    "    return feat_sel.reshape(sel_shape, -1)\n",
    "\n",
    "# At the beginning and end of each run, a fixation cross was shown for 6 sec (3TORs) and\n",
    "# 12 sec (6TORs), respectively. hence stIndex goes from 3:-6\n",
    "# 37 images shows in each run >> 185 TOR\n",
    "# Each image was presented for 1 sec followed by a 9 sec fixation cross (5TORs)\n",
    "# For each stimuls, average assocated 5 TORs and map them\n",
    "def loadFmriData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4, img_data.shape[-1] - 5, 5):\n",
    "        #x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "        x_train.append((img_data[:,:,:,stIndex+3]))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    return x\n",
    "\n",
    "def loadFmriLstmData(file_path):\n",
    "    x_train = []\n",
    "    epi_img = nib.load(file_path)\n",
    "    img_data = epi_img.get_fdata()\n",
    "    for stIndex in range(4,  img_data.shape[-1] - 5, 5):\n",
    "        x_train.append(np.mean(img_data[:,:,:,stIndex:stIndex+5], axis=-1))\n",
    "\n",
    "    x = np.asarray(x_train)\n",
    "    #(37, 106, 106, 69) > (37, 69, 106*106)\n",
    "    x = np.swapaxes(np.reshape(x, (37, -1, 69)), 1, 2)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def featureVectorLoader(data_split, data_type, filterNum):\n",
    "    #every file has 35 feature vectors (one batch)\n",
    "    L = len(fileList)   \n",
    "    x_images = data_split.get(data_type, None)\n",
    "    while True:\n",
    "        for participant, sessDict in x_images.items():\n",
    "            for sess, runDict in sessDict.items():\n",
    "                for run in runDict.keys():\n",
    "                    #fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % participant, \"sess\" \"_\".join([participant, sess, run]) + \".npy\")\n",
    "                    feature_file_name = \"_\".join([participant, sess, run]) + \".npy\"\n",
    "                    #sub-CSI3/ses-01/func\n",
    "                    # sub-CSI3_ses-09_task-5000scenes_run-05_bold.nii.gz\n",
    "                    match = fmriRegex.match(feature_file_name)\n",
    "                    if match:\n",
    "                        fmri_file_name = \"sub-%s_ses-%s_task-5000scenes_run-%s_bold.nii.gz\" % ( match.group(1), match.group(2), match.group(3))\n",
    "                        fmri_data_path = os.path.join(fmri_data_dir, \"sub-%s\" % match.group(1), \"ses-%s\" % match.group(2), \"func\", fmri_file_name)\n",
    "                \n",
    "                    feature_vector_path= os.path.join(stimuli_features_dir, feature_file_name)\n",
    "                    X = loadFmriLstmData(fmri_data_path)\n",
    "                    Y = loadFilterVector(feature_vector_path, filterNum)\n",
    "                    yield (X,Y)\n",
    "\n",
    "\n",
    "# for each y, we have \n",
    "def auto_encoder(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = Conv2D(2, (5,5), activation='relu')(X_input)\n",
    "    #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    #X = Dropout(0.25)(X)\n",
    "    X = Conv2D(4, (1,1), activation='tanh')(X_input)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(encoding_dim, activation='relu')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder')\n",
    "    return model\n",
    "\n",
    "# for each y, we have \n",
    "def auto_encoder_lstm(input_shape, encoding_dim):\n",
    "    X_input = Input(input_shape)\n",
    "    #X = LSTM(units = 128, return_sequences = True)(X_input)\n",
    "    #LSTM(128, dropout=0.2, recurrent_dropout=0.2)\n",
    "    X = LSTM(128)(X_input)\n",
    "    #X = TimeDistributed(Dense(encoding_dim, activation = \"sigmoid\"))(X)\n",
    "    X = Dense(encoding_dim, activation = \"sigmoid\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='auto_encoder_lstm')\n",
    "    return model\n",
    "\n",
    "EPOCHS = 100\n",
    "filterNumList = [452, 209, 327, 377, 33, 16, 433, 19, 66, 467]\n",
    "#filterNumList = [377]\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "encoder_model = auto_encoder_lstm((69, 106*106), len(filterNumList)*144)\n",
    "#encoder_model = auto_encoder((106,106,69), len(filterNumList)*144)\n",
    "#cosine_proximity\n",
    "#encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "encoder_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "train_history = encoder_model.fit_generator(featureVectorLoader(x_images_path, \"train\", filterNumList), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                                    validation_data=featureVectorLoader(x_images_path, \"dev\", filterNumList),\n",
    "                                    validation_steps=validation_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = unrollContentOutput(predictContentOut)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Input image shape:', x.shape)\n",
    "print(model.predict(x))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed fmri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "def dnn_classifier(input_shape, num_classes):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(512, activation='tanh')(X)\n",
    "    X = Dense(128, activation='tanh')(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='dnn_classifier')\n",
    "    return model\n",
    "\n",
    "input_shape = x_content.shape[1:]\n",
    "model2 = dnn_classifier(input_shape, num_classes)\n",
    "\n",
    "EPOCHS=100\n",
    "#callbacks\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "#             ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath='weights.{epoch:02d}.h5', monitor='val_loss', verbose=1)]\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=5, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test)) \n",
    "\n",
    "#steps_per_epoch = (last_sess - 1) * (last_run - 1)\n",
    "\n",
    "numberOfSessions = data_split[\"train\"][\"last_sess\"] - data_split[\"train\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"train\"][\"last_run\"] - data_split[\"train\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"train\"][\"participant_list\"])\n",
    "steps_per_epoch = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "numberOfSessions = data_split[\"dev\"][\"last_sess\"] - data_split[\"dev\"][\"start_sess\"]\n",
    "numberOfRuns = data_split[\"dev\"][\"last_run\"] - data_split[\"dev\"][\"start_run\"]\n",
    "numberOfParticipants = len(data_split[\"dev\"][\"participant_list\"])\n",
    "validation_steps = numberOfSessions * numberOfRuns * numberOfParticipants\n",
    "\n",
    "\n",
    "print(\"Total number of training examples: %s\" % (steps_per_epoch * 37))\n",
    "print(\"Total number of dev examples: %s\" % (validation_steps * 37))\n",
    "\n",
    "print(\"steps_per_epoch: %s\" % steps_per_epoch)\n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path), steps_per_epoch=350, epochs=EPOCHS, validation_data=(x_test, y_test)) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=350, epochs=EPOCHS, validation_data=featureVectorLoader(x_images_path, \"train\"), validation_steps=350) \n",
    "#train_history = model.fit_generator(featureVectorLoader(x_images_path, \"train\"), steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "#                                    callbacks=callbacks, validation_data=featureVectorLoader(x_images_path, \"dev\"),\n",
    "#                                    validation_steps=validation_steps) \n",
    "train_history = model2.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, batch_size=35, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = train_history.history['loss']\n",
    "val_loss = train_history.history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
