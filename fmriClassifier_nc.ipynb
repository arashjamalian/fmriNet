{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50, VGG19\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import nibabel as nib\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "SEED=1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "#tf.set_random_seed(SEED)\n",
    "tf.random.set_seed\n",
    "\n",
    "K.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 200)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 131)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 143)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 86)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 187)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 101)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 152)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 190)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 285)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 210)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 200)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 131)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 143)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 86)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 187)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 101)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 152)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 190)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 285)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 210)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 200)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 131)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 143)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 86)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 187)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 101)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 152)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 190)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 285)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 210)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 200)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 131)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 143)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 86)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 187)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 101)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 152)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 190)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 285)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 210)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 200)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 131)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 143)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 86)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 187)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 101)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 152)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 190)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 285)\n",
      "['LHEarlyVis', 'LHLOC', 'LHOPA', 'LHPPA', 'LHRSC', 'RHEarlyVis', 'RHLOC', 'RHOPA', 'RHPPA', 'RHRSC']\n",
      "(5254, 210)\n",
      "tr: 0, roi: 0 shape: (5254, 285)\n",
      "tr: 0, roi: 1 shape: (5254, 285)\n",
      "tr: 0, roi: 2 shape: (5254, 285)\n",
      "tr: 0, roi: 3 shape: (5254, 285)\n",
      "tr: 0, roi: 4 shape: (5254, 285)\n",
      "tr: 0, roi: 5 shape: (5254, 285)\n",
      "tr: 0, roi: 6 shape: (5254, 285)\n",
      "tr: 0, roi: 7 shape: (5254, 285)\n",
      "tr: 0, roi: 8 shape: (5254, 285)\n",
      "tr: 0, roi: 9 shape: (5254, 285)\n",
      "tr: 1, roi: 0 shape: (5254, 285)\n",
      "tr: 1, roi: 1 shape: (5254, 285)\n",
      "tr: 1, roi: 2 shape: (5254, 285)\n",
      "tr: 1, roi: 3 shape: (5254, 285)\n",
      "tr: 1, roi: 4 shape: (5254, 285)\n",
      "tr: 1, roi: 5 shape: (5254, 285)\n",
      "tr: 1, roi: 6 shape: (5254, 285)\n",
      "tr: 1, roi: 7 shape: (5254, 285)\n",
      "tr: 1, roi: 8 shape: (5254, 285)\n",
      "tr: 1, roi: 9 shape: (5254, 285)\n",
      "tr: 2, roi: 0 shape: (5254, 285)\n",
      "tr: 2, roi: 1 shape: (5254, 285)\n",
      "tr: 2, roi: 2 shape: (5254, 285)\n",
      "tr: 2, roi: 3 shape: (5254, 285)\n",
      "tr: 2, roi: 4 shape: (5254, 285)\n",
      "tr: 2, roi: 5 shape: (5254, 285)\n",
      "tr: 2, roi: 6 shape: (5254, 285)\n",
      "tr: 2, roi: 7 shape: (5254, 285)\n",
      "tr: 2, roi: 8 shape: (5254, 285)\n",
      "tr: 2, roi: 9 shape: (5254, 285)\n",
      "tr: 3, roi: 0 shape: (5254, 285)\n",
      "tr: 3, roi: 1 shape: (5254, 285)\n",
      "tr: 3, roi: 2 shape: (5254, 285)\n",
      "tr: 3, roi: 3 shape: (5254, 285)\n",
      "tr: 3, roi: 4 shape: (5254, 285)\n",
      "tr: 3, roi: 5 shape: (5254, 285)\n",
      "tr: 3, roi: 6 shape: (5254, 285)\n",
      "tr: 3, roi: 7 shape: (5254, 285)\n",
      "tr: 3, roi: 8 shape: (5254, 285)\n",
      "tr: 3, roi: 9 shape: (5254, 285)\n",
      "tr: 4, roi: 0 shape: (5254, 285)\n",
      "tr: 4, roi: 1 shape: (5254, 285)\n",
      "tr: 4, roi: 2 shape: (5254, 285)\n",
      "tr: 4, roi: 3 shape: (5254, 285)\n",
      "tr: 4, roi: 4 shape: (5254, 285)\n",
      "tr: 4, roi: 5 shape: (5254, 285)\n",
      "tr: 4, roi: 6 shape: (5254, 285)\n",
      "tr: 4, roi: 7 shape: (5254, 285)\n",
      "tr: 4, roi: 8 shape: (5254, 285)\n",
      "tr: 4, roi: 9 shape: (5254, 285)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get fmri ROI data\n",
    "import h5py\n",
    "tr_list = []\n",
    "ax_length = 285\n",
    "for TR in ['TR1','TR2','TR3','TR4','TR5']:\n",
    "    roi_list = []\n",
    "    for roi in ['RHPPA', 'LHPPA', 'RHRSC', 'LHRSC', 'RHOPA', 'LHOPA', 'LHLOC', 'RHLOC', 'RHEarlyVis', 'LHEarlyVis']:\n",
    "        with h5py.File('/home/ubuntu/boldroi/ROIs/CSI1/h5/CSI1_ROIs_%s.h5' % TR, 'r') as f:\n",
    "            print(list(f.keys()))\n",
    "            x_roi = list(f[roi])\n",
    "            x_r = np.stack(x_roi, axis=0)\n",
    "            print(x_r.shape)\n",
    "            x_r = np.pad(x_r, ((0, 0), (0, ax_length-x_r.shape[1])), mode='constant', constant_values=0)\n",
    "            roi_list.append(x_r)\n",
    "\n",
    "    tr_list.append(roi_list)\n",
    "\n",
    "#x_all = np.concatenate(roi_list)\n",
    "#x_all =np.squeeze(x_all)\n",
    "#print(x_all.shape)\n",
    "for tr_num, roi_list in enumerate(tr_list):\n",
    "    for roi_num, roi in enumerate(roi_list):\n",
    "        print(\"tr: %s, roi: %s shape: %s\" % (tr_num, roi_num, str(roi.shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_num: 0\n",
      "(5254, 2850)\n",
      "tr_num: 1\n",
      "(5254, 2850)\n",
      "tr_num: 2\n",
      "(5254, 2850)\n",
      "tr_num: 3\n",
      "(5254, 2850)\n",
      "tr_num: 4\n",
      "(5254, 2850)\n"
     ]
    }
   ],
   "source": [
    "xtr_list = []\n",
    "for tr_num, rlist in enumerate(tr_list):\n",
    "    print(\"tr_num: %s\" % tr_num)\n",
    "    xa = np.hstack(rlist)\n",
    "    xtr_list.append(xa)\n",
    "    print(xa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5254, 5, 2850)\n"
     ]
    }
   ],
   "source": [
    "x_all = np.stack(xtr_list, axis=1)\n",
    "print(x_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('CSI1_labels_all.npy', 'rb') as f:\n",
    "    labels_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (4203, 5, 2850)\n",
      "y train shape: (4203, 3)\n",
      "x test shape: (1051, 5, 2850)\n",
      "y test shape: (1051, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/test set\n",
    "from sklearn.utils import shuffle\n",
    "num_samples = x_all.shape[0]\n",
    "div = int(num_samples * 0.8)\n",
    "x_shuffle, y_shuffle = shuffle(x_all, labels_all, random_state=0)\n",
    "x_train = x_shuffle[0:div, :]\n",
    "y_train = y_shuffle[0:div]\n",
    "x_test = x_shuffle[div:, :]\n",
    "y_test = y_shuffle[div:]\n",
    "\n",
    "#x_train = x_all[0:div, :]\n",
    "#y_train = labels_all[0:div]\n",
    "#x_test = x_all[div:, :]\n",
    "#y_test = labels_all[div:]\n",
    "\n",
    "#num_samples = x_all.shape[0]\n",
    "##num_voxels = 100  # maximum number of voxels to use for training\n",
    "#div = int(num_samples * 0.6)\n",
    "#print(\"Division index: %s\" % div)\n",
    "##x_shuffle, y_shuffle = shuffle(x_all, y_all, random_state=0)\n",
    "#x_train = x_all[0:div, :]\n",
    "##y_train = y_all[0:div, :]\n",
    "#y_train = labels_all[0:div]\n",
    "#\n",
    "#x_test = x_all[div:, :]\n",
    "##y_test = y_all[div:, :]\n",
    "#y_test = labels_all[div:]\n",
    "print(\"x train shape: %s\" % str(x_train.shape))\n",
    "print(\"y train shape: %s\" % str(y_train.shape))\n",
    "print(\"x test shape: %s\" % str(x_test.shape))\n",
    "print(\"y test shape: %s\" % str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 5, 2850)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 64)                746240    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 748,611\n",
      "Trainable params: 748,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "del classifier_model\n",
    "X_input = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "#X = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(X_input)\n",
    "#X = TimeDistributed(Dense(32, activation='relu'))(X_input)\n",
    "X = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(X_input)\n",
    "X = Dense(32, activation='relu')(X)\n",
    "X = Dense(8, activation='relu')(X)\n",
    "predictions = Dense(3, activation='softmax')(X)\n",
    "\n",
    "#X_input = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "##X = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(X_input)\n",
    "#X = LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(X_input)\n",
    "#X = TimeDistributed(Dense(32, activation='relu'))(X)\n",
    "#X = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(X)\n",
    "#X = Dropout(0.1)(X)\n",
    "#X = Dense(8, activation='relu')(X)\n",
    "#X = Dropout(0.1)(X)\n",
    "#predictions = Dense(3, activation='softmax')(X)\n",
    "\n",
    "#X_input = Input(shape=(x_train.shape[1]))\n",
    "#X = Dense(4, activation='relu')(X_input)\n",
    "#X = Dropout(0.2)(X)\n",
    "#X = Dense(32, activation='relu')(X)\n",
    "#X = Dropout(0.2)(X)\n",
    "#X = Dense(32, activation='relu')(X)\n",
    "#X = Dropout(0.2)(X)\n",
    "#X = Dense(32, activation='relu')(X)\n",
    "#X = Dropout(0.2)(X)\n",
    "#predictions = Dense(3, activation='softmax')(X)\n",
    "\n",
    "classifier_model = Model(inputs=X_input, outputs=predictions)\n",
    "classifier_model.summary()\n",
    "\n",
    "\n",
    "i=0\n",
    "callbacks = [TensorBoard(log_dir=f'./log/{i}'), EarlyStopping(monitor='val_loss', mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4203 samples, validate on 1051 samples\n",
      "Epoch 1/150\n",
      "4203/4203 [==============================] - 5s 1ms/sample - loss: 1.0887 - accuracy: 0.4059 - val_loss: 1.0715 - val_accuracy: 0.4377\n",
      "Epoch 2/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 1.0553 - accuracy: 0.4278 - val_loss: 1.0250 - val_accuracy: 0.4367\n",
      "Epoch 3/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 1.0197 - accuracy: 0.4356 - val_loss: 0.9900 - val_accuracy: 0.4586\n",
      "Epoch 4/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.9945 - accuracy: 0.4585 - val_loss: 0.9675 - val_accuracy: 0.5014\n",
      "Epoch 5/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.9724 - accuracy: 0.4854 - val_loss: 0.9486 - val_accuracy: 0.5233\n",
      "Epoch 6/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.9502 - accuracy: 0.5018 - val_loss: 0.9316 - val_accuracy: 0.5528\n",
      "Epoch 7/150\n",
      "4203/4203 [==============================] - 3s 731us/sample - loss: 0.9276 - accuracy: 0.5211 - val_loss: 0.9141 - val_accuracy: 0.5728\n",
      "Epoch 8/150\n",
      "4203/4203 [==============================] - 3s 747us/sample - loss: 0.9083 - accuracy: 0.5270 - val_loss: 0.9000 - val_accuracy: 0.5794\n",
      "Epoch 9/150\n",
      "4203/4203 [==============================] - 3s 736us/sample - loss: 0.8902 - accuracy: 0.5453 - val_loss: 0.8878 - val_accuracy: 0.5794\n",
      "Epoch 10/150\n",
      "4203/4203 [==============================] - 3s 736us/sample - loss: 0.8743 - accuracy: 0.5565 - val_loss: 0.8767 - val_accuracy: 0.5804\n",
      "Epoch 11/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.8613 - accuracy: 0.5691 - val_loss: 0.8677 - val_accuracy: 0.5909\n",
      "Epoch 12/150\n",
      "4203/4203 [==============================] - 3s 738us/sample - loss: 0.8523 - accuracy: 0.5732 - val_loss: 0.8618 - val_accuracy: 0.6051\n",
      "Epoch 13/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.8408 - accuracy: 0.5851 - val_loss: 0.8566 - val_accuracy: 0.6061\n",
      "Epoch 14/150\n",
      "4203/4203 [==============================] - 3s 755us/sample - loss: 0.8355 - accuracy: 0.5789 - val_loss: 0.8500 - val_accuracy: 0.6061\n",
      "Epoch 15/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 0.8271 - accuracy: 0.5934 - val_loss: 0.8481 - val_accuracy: 0.6013\n",
      "Epoch 16/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.8224 - accuracy: 0.5991 - val_loss: 0.8451 - val_accuracy: 0.6013\n",
      "Epoch 17/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.8183 - accuracy: 0.5962 - val_loss: 0.8421 - val_accuracy: 0.6032\n",
      "Epoch 18/150\n",
      "4203/4203 [==============================] - 3s 753us/sample - loss: 0.8129 - accuracy: 0.6029 - val_loss: 0.8396 - val_accuracy: 0.6004\n",
      "Epoch 19/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.8064 - accuracy: 0.6062 - val_loss: 0.8377 - val_accuracy: 0.6080\n",
      "Epoch 20/150\n",
      "4203/4203 [==============================] - 3s 753us/sample - loss: 0.8075 - accuracy: 0.6034 - val_loss: 0.8359 - val_accuracy: 0.6042\n",
      "Epoch 21/150\n",
      "4203/4203 [==============================] - 3s 753us/sample - loss: 0.8045 - accuracy: 0.6072 - val_loss: 0.8344 - val_accuracy: 0.6080\n",
      "Epoch 22/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.8003 - accuracy: 0.6143 - val_loss: 0.8329 - val_accuracy: 0.6099\n",
      "Epoch 23/150\n",
      "4203/4203 [==============================] - 3s 755us/sample - loss: 0.7997 - accuracy: 0.6072 - val_loss: 0.8327 - val_accuracy: 0.6099\n",
      "Epoch 24/150\n",
      "4203/4203 [==============================] - 3s 752us/sample - loss: 0.7952 - accuracy: 0.6141 - val_loss: 0.8315 - val_accuracy: 0.6118\n",
      "Epoch 25/150\n",
      "4203/4203 [==============================] - 3s 784us/sample - loss: 0.7922 - accuracy: 0.6091 - val_loss: 0.8320 - val_accuracy: 0.6089\n",
      "Epoch 26/150\n",
      "4203/4203 [==============================] - 4s 901us/sample - loss: 0.7906 - accuracy: 0.6138 - val_loss: 0.8310 - val_accuracy: 0.6108\n",
      "Epoch 27/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7866 - accuracy: 0.6212 - val_loss: 0.8303 - val_accuracy: 0.6108\n",
      "Epoch 28/150\n",
      "4203/4203 [==============================] - 3s 735us/sample - loss: 0.7847 - accuracy: 0.6255 - val_loss: 0.8294 - val_accuracy: 0.6080\n",
      "Epoch 29/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7814 - accuracy: 0.6241 - val_loss: 0.8296 - val_accuracy: 0.6070\n",
      "Epoch 30/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.7825 - accuracy: 0.6198 - val_loss: 0.8272 - val_accuracy: 0.6089\n",
      "Epoch 31/150\n",
      "4203/4203 [==============================] - 3s 754us/sample - loss: 0.7731 - accuracy: 0.6286 - val_loss: 0.8280 - val_accuracy: 0.6156\n",
      "Epoch 32/150\n",
      "4203/4203 [==============================] - 3s 740us/sample - loss: 0.7772 - accuracy: 0.6310 - val_loss: 0.8262 - val_accuracy: 0.6147\n",
      "Epoch 33/150\n",
      "4203/4203 [==============================] - 3s 751us/sample - loss: 0.7800 - accuracy: 0.6215 - val_loss: 0.8264 - val_accuracy: 0.6147\n",
      "Epoch 34/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7739 - accuracy: 0.6300 - val_loss: 0.8259 - val_accuracy: 0.6127\n",
      "Epoch 35/150\n",
      "4203/4203 [==============================] - 3s 750us/sample - loss: 0.7696 - accuracy: 0.6341 - val_loss: 0.8260 - val_accuracy: 0.6147\n",
      "Epoch 36/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7706 - accuracy: 0.6307 - val_loss: 0.8253 - val_accuracy: 0.6175\n",
      "Epoch 37/150\n",
      "4203/4203 [==============================] - 3s 732us/sample - loss: 0.7715 - accuracy: 0.6434 - val_loss: 0.8254 - val_accuracy: 0.6108\n",
      "Epoch 38/150\n",
      "4203/4203 [==============================] - 3s 730us/sample - loss: 0.7725 - accuracy: 0.6286 - val_loss: 0.8253 - val_accuracy: 0.6147\n",
      "Epoch 39/150\n",
      "4203/4203 [==============================] - 3s 740us/sample - loss: 0.7663 - accuracy: 0.6362 - val_loss: 0.8250 - val_accuracy: 0.6137\n",
      "Epoch 40/150\n",
      "4203/4203 [==============================] - 3s 738us/sample - loss: 0.7666 - accuracy: 0.6357 - val_loss: 0.8241 - val_accuracy: 0.6166\n",
      "Epoch 41/150\n",
      "4203/4203 [==============================] - 3s 754us/sample - loss: 0.7647 - accuracy: 0.6379 - val_loss: 0.8244 - val_accuracy: 0.6166\n",
      "Epoch 42/150\n",
      "4203/4203 [==============================] - 3s 751us/sample - loss: 0.7647 - accuracy: 0.6348 - val_loss: 0.8231 - val_accuracy: 0.6185\n",
      "Epoch 43/150\n",
      "4203/4203 [==============================] - 3s 755us/sample - loss: 0.7588 - accuracy: 0.6338 - val_loss: 0.8224 - val_accuracy: 0.6137\n",
      "Epoch 44/150\n",
      "4203/4203 [==============================] - 3s 751us/sample - loss: 0.7615 - accuracy: 0.6365 - val_loss: 0.8230 - val_accuracy: 0.6166\n",
      "Epoch 45/150\n",
      "4203/4203 [==============================] - 3s 744us/sample - loss: 0.7580 - accuracy: 0.6388 - val_loss: 0.8220 - val_accuracy: 0.6137\n",
      "Epoch 46/150\n",
      "4203/4203 [==============================] - 3s 730us/sample - loss: 0.7617 - accuracy: 0.6343 - val_loss: 0.8226 - val_accuracy: 0.6166\n",
      "Epoch 47/150\n",
      "4203/4203 [==============================] - 3s 734us/sample - loss: 0.7569 - accuracy: 0.6441 - val_loss: 0.8225 - val_accuracy: 0.6194\n",
      "Epoch 48/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.7575 - accuracy: 0.6362 - val_loss: 0.8224 - val_accuracy: 0.6156\n",
      "Epoch 49/150\n",
      "4203/4203 [==============================] - 3s 733us/sample - loss: 0.7490 - accuracy: 0.6455 - val_loss: 0.8221 - val_accuracy: 0.6147\n",
      "Epoch 50/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7489 - accuracy: 0.6395 - val_loss: 0.8219 - val_accuracy: 0.6194\n",
      "Epoch 51/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7483 - accuracy: 0.6467 - val_loss: 0.8217 - val_accuracy: 0.6213\n",
      "Epoch 52/150\n",
      "4203/4203 [==============================] - 3s 733us/sample - loss: 0.7494 - accuracy: 0.6457 - val_loss: 0.8214 - val_accuracy: 0.6223\n",
      "Epoch 53/150\n",
      "4203/4203 [==============================] - 3s 734us/sample - loss: 0.7521 - accuracy: 0.6429 - val_loss: 0.8208 - val_accuracy: 0.6194\n",
      "Epoch 54/150\n",
      "4203/4203 [==============================] - 3s 735us/sample - loss: 0.7484 - accuracy: 0.6455 - val_loss: 0.8205 - val_accuracy: 0.6137\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4203/4203 [==============================] - 3s 738us/sample - loss: 0.7485 - accuracy: 0.6464 - val_loss: 0.8206 - val_accuracy: 0.6204\n",
      "Epoch 56/150\n",
      "4203/4203 [==============================] - 3s 729us/sample - loss: 0.7472 - accuracy: 0.6469 - val_loss: 0.8205 - val_accuracy: 0.6194\n",
      "Epoch 57/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 0.7461 - accuracy: 0.6507 - val_loss: 0.8203 - val_accuracy: 0.6175\n",
      "Epoch 58/150\n",
      "4203/4203 [==============================] - 3s 733us/sample - loss: 0.7510 - accuracy: 0.6469 - val_loss: 0.8198 - val_accuracy: 0.6185\n",
      "Epoch 59/150\n",
      "4203/4203 [==============================] - 3s 735us/sample - loss: 0.7455 - accuracy: 0.6410 - val_loss: 0.8199 - val_accuracy: 0.6185\n",
      "Epoch 60/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7407 - accuracy: 0.6479 - val_loss: 0.8210 - val_accuracy: 0.6194\n",
      "Epoch 61/150\n",
      "4203/4203 [==============================] - 3s 758us/sample - loss: 0.7383 - accuracy: 0.6531 - val_loss: 0.8205 - val_accuracy: 0.6204\n",
      "Epoch 62/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7450 - accuracy: 0.6419 - val_loss: 0.8204 - val_accuracy: 0.6204\n",
      "Epoch 63/150\n",
      "4203/4203 [==============================] - 3s 730us/sample - loss: 0.7414 - accuracy: 0.6464 - val_loss: 0.8203 - val_accuracy: 0.6213\n",
      "Epoch 64/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 0.7387 - accuracy: 0.6524 - val_loss: 0.8202 - val_accuracy: 0.6194\n",
      "Epoch 65/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.7383 - accuracy: 0.6536 - val_loss: 0.8201 - val_accuracy: 0.6223\n",
      "Epoch 66/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7363 - accuracy: 0.6498 - val_loss: 0.8202 - val_accuracy: 0.6242\n",
      "Epoch 67/150\n",
      "4203/4203 [==============================] - 3s 751us/sample - loss: 0.7393 - accuracy: 0.6502 - val_loss: 0.8197 - val_accuracy: 0.6232\n",
      "Epoch 68/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7370 - accuracy: 0.6453 - val_loss: 0.8199 - val_accuracy: 0.6251\n",
      "Epoch 69/150\n",
      "4203/4203 [==============================] - 3s 744us/sample - loss: 0.7393 - accuracy: 0.6507 - val_loss: 0.8195 - val_accuracy: 0.6251\n",
      "Epoch 70/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 0.7368 - accuracy: 0.6541 - val_loss: 0.8195 - val_accuracy: 0.6242\n",
      "Epoch 71/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.7407 - accuracy: 0.6493 - val_loss: 0.8198 - val_accuracy: 0.6251\n",
      "Epoch 72/150\n",
      "4203/4203 [==============================] - 3s 757us/sample - loss: 0.7327 - accuracy: 0.6552 - val_loss: 0.8191 - val_accuracy: 0.6251\n",
      "Epoch 73/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7332 - accuracy: 0.6524 - val_loss: 0.8196 - val_accuracy: 0.6251\n",
      "Epoch 74/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7325 - accuracy: 0.6557 - val_loss: 0.8200 - val_accuracy: 0.6232\n",
      "Epoch 75/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7331 - accuracy: 0.6481 - val_loss: 0.8197 - val_accuracy: 0.6232\n",
      "Epoch 76/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7283 - accuracy: 0.6588 - val_loss: 0.8199 - val_accuracy: 0.6242\n",
      "Epoch 77/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7306 - accuracy: 0.6548 - val_loss: 0.8199 - val_accuracy: 0.6242\n",
      "Epoch 78/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7303 - accuracy: 0.6571 - val_loss: 0.8196 - val_accuracy: 0.6242\n",
      "Epoch 79/150\n",
      "4203/4203 [==============================] - 3s 754us/sample - loss: 0.7327 - accuracy: 0.6502 - val_loss: 0.8198 - val_accuracy: 0.6251\n",
      "Epoch 80/150\n",
      "4203/4203 [==============================] - 3s 757us/sample - loss: 0.7306 - accuracy: 0.6557 - val_loss: 0.8197 - val_accuracy: 0.6232\n",
      "Epoch 81/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.7301 - accuracy: 0.6633 - val_loss: 0.8193 - val_accuracy: 0.6242\n",
      "Epoch 82/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7293 - accuracy: 0.6610 - val_loss: 0.8199 - val_accuracy: 0.6261\n",
      "Epoch 83/150\n",
      "4203/4203 [==============================] - 4s 919us/sample - loss: 0.7255 - accuracy: 0.6600 - val_loss: 0.8197 - val_accuracy: 0.6251\n",
      "Epoch 84/150\n",
      "4203/4203 [==============================] - 3s 763us/sample - loss: 0.7302 - accuracy: 0.6548 - val_loss: 0.8195 - val_accuracy: 0.6261\n",
      "Epoch 85/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7246 - accuracy: 0.6567 - val_loss: 0.8199 - val_accuracy: 0.6261\n",
      "Epoch 86/150\n",
      "4203/4203 [==============================] - 3s 747us/sample - loss: 0.7231 - accuracy: 0.6693 - val_loss: 0.8196 - val_accuracy: 0.6242\n",
      "Epoch 87/150\n",
      "4203/4203 [==============================] - 3s 756us/sample - loss: 0.7315 - accuracy: 0.6483 - val_loss: 0.8195 - val_accuracy: 0.6232\n",
      "Epoch 88/150\n",
      "4203/4203 [==============================] - 3s 750us/sample - loss: 0.7251 - accuracy: 0.6550 - val_loss: 0.8190 - val_accuracy: 0.6270\n",
      "Epoch 89/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7191 - accuracy: 0.6650 - val_loss: 0.8197 - val_accuracy: 0.6261\n",
      "Epoch 90/150\n",
      "4203/4203 [==============================] - 3s 755us/sample - loss: 0.7275 - accuracy: 0.6650 - val_loss: 0.8201 - val_accuracy: 0.6270\n",
      "Epoch 91/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7252 - accuracy: 0.6588 - val_loss: 0.8194 - val_accuracy: 0.6261\n",
      "Epoch 92/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7237 - accuracy: 0.6600 - val_loss: 0.8195 - val_accuracy: 0.6251\n",
      "Epoch 93/150\n",
      "4203/4203 [==============================] - 3s 755us/sample - loss: 0.7211 - accuracy: 0.6657 - val_loss: 0.8194 - val_accuracy: 0.6261\n",
      "Epoch 94/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.7215 - accuracy: 0.6636 - val_loss: 0.8197 - val_accuracy: 0.6261\n",
      "Epoch 95/150\n",
      "4203/4203 [==============================] - 3s 740us/sample - loss: 0.7215 - accuracy: 0.6629 - val_loss: 0.8201 - val_accuracy: 0.6261\n",
      "Epoch 96/150\n",
      "4203/4203 [==============================] - 3s 740us/sample - loss: 0.7144 - accuracy: 0.6640 - val_loss: 0.8202 - val_accuracy: 0.6270\n",
      "Epoch 97/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7205 - accuracy: 0.6657 - val_loss: 0.8196 - val_accuracy: 0.6280\n",
      "Epoch 98/150\n",
      "4203/4203 [==============================] - 3s 762us/sample - loss: 0.7192 - accuracy: 0.6629 - val_loss: 0.8202 - val_accuracy: 0.6270\n",
      "Epoch 99/150\n",
      "4203/4203 [==============================] - 3s 758us/sample - loss: 0.7258 - accuracy: 0.6621 - val_loss: 0.8201 - val_accuracy: 0.6251\n",
      "Epoch 100/150\n",
      "4203/4203 [==============================] - 3s 751us/sample - loss: 0.7142 - accuracy: 0.6617 - val_loss: 0.8203 - val_accuracy: 0.6280\n",
      "Epoch 101/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.7162 - accuracy: 0.6631 - val_loss: 0.8196 - val_accuracy: 0.6251\n",
      "Epoch 102/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.7203 - accuracy: 0.6638 - val_loss: 0.8192 - val_accuracy: 0.6270\n",
      "Epoch 103/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7188 - accuracy: 0.6688 - val_loss: 0.8193 - val_accuracy: 0.6261\n",
      "Epoch 104/150\n",
      "4203/4203 [==============================] - 3s 756us/sample - loss: 0.7177 - accuracy: 0.6614 - val_loss: 0.8198 - val_accuracy: 0.6261\n",
      "Epoch 105/150\n",
      "4203/4203 [==============================] - 3s 747us/sample - loss: 0.7123 - accuracy: 0.6698 - val_loss: 0.8205 - val_accuracy: 0.6280\n",
      "Epoch 106/150\n",
      "4203/4203 [==============================] - 3s 750us/sample - loss: 0.7085 - accuracy: 0.6724 - val_loss: 0.8202 - val_accuracy: 0.6242\n",
      "Epoch 107/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.7148 - accuracy: 0.6640 - val_loss: 0.8198 - val_accuracy: 0.6261\n",
      "Epoch 108/150\n",
      "4203/4203 [==============================] - 3s 744us/sample - loss: 0.7129 - accuracy: 0.6660 - val_loss: 0.8202 - val_accuracy: 0.6289\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4203/4203 [==============================] - 3s 735us/sample - loss: 0.7192 - accuracy: 0.6576 - val_loss: 0.8201 - val_accuracy: 0.6289\n",
      "Epoch 110/150\n",
      "4203/4203 [==============================] - 3s 753us/sample - loss: 0.7145 - accuracy: 0.6702 - val_loss: 0.8196 - val_accuracy: 0.6242\n",
      "Epoch 111/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7140 - accuracy: 0.6621 - val_loss: 0.8199 - val_accuracy: 0.6270\n",
      "Epoch 112/150\n",
      "4203/4203 [==============================] - 3s 747us/sample - loss: 0.7124 - accuracy: 0.6636 - val_loss: 0.8199 - val_accuracy: 0.6280\n",
      "Epoch 113/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.7121 - accuracy: 0.6679 - val_loss: 0.8204 - val_accuracy: 0.6289\n",
      "Epoch 114/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7163 - accuracy: 0.6705 - val_loss: 0.8198 - val_accuracy: 0.6289\n",
      "Epoch 115/150\n",
      "4203/4203 [==============================] - 3s 753us/sample - loss: 0.7177 - accuracy: 0.6667 - val_loss: 0.8201 - val_accuracy: 0.6280\n",
      "Epoch 116/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7073 - accuracy: 0.6614 - val_loss: 0.8200 - val_accuracy: 0.6299\n",
      "Epoch 117/150\n",
      "4203/4203 [==============================] - 3s 756us/sample - loss: 0.7145 - accuracy: 0.6733 - val_loss: 0.8196 - val_accuracy: 0.6289\n",
      "Epoch 118/150\n",
      "4203/4203 [==============================] - 3s 760us/sample - loss: 0.7170 - accuracy: 0.6693 - val_loss: 0.8200 - val_accuracy: 0.6299\n",
      "Epoch 119/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.7089 - accuracy: 0.6712 - val_loss: 0.8200 - val_accuracy: 0.6270\n",
      "Epoch 120/150\n",
      "4203/4203 [==============================] - 3s 758us/sample - loss: 0.7076 - accuracy: 0.6702 - val_loss: 0.8199 - val_accuracy: 0.6299\n",
      "Epoch 121/150\n",
      "4203/4203 [==============================] - 3s 752us/sample - loss: 0.7140 - accuracy: 0.6643 - val_loss: 0.8202 - val_accuracy: 0.6299\n",
      "Epoch 122/150\n",
      "4203/4203 [==============================] - 3s 759us/sample - loss: 0.7025 - accuracy: 0.6748 - val_loss: 0.8204 - val_accuracy: 0.6299\n",
      "Epoch 123/150\n",
      "4203/4203 [==============================] - 3s 760us/sample - loss: 0.7094 - accuracy: 0.6676 - val_loss: 0.8202 - val_accuracy: 0.6299\n",
      "Epoch 124/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7104 - accuracy: 0.6624 - val_loss: 0.8199 - val_accuracy: 0.6327\n",
      "Epoch 125/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.7036 - accuracy: 0.6717 - val_loss: 0.8202 - val_accuracy: 0.6318\n",
      "Epoch 126/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.7062 - accuracy: 0.6738 - val_loss: 0.8209 - val_accuracy: 0.6261\n",
      "Epoch 127/150\n",
      "4203/4203 [==============================] - 3s 756us/sample - loss: 0.7079 - accuracy: 0.6617 - val_loss: 0.8211 - val_accuracy: 0.6261\n",
      "Epoch 128/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7036 - accuracy: 0.6707 - val_loss: 0.8205 - val_accuracy: 0.6280\n",
      "Epoch 129/150\n",
      "4203/4203 [==============================] - 3s 754us/sample - loss: 0.7100 - accuracy: 0.6686 - val_loss: 0.8206 - val_accuracy: 0.6270\n",
      "Epoch 130/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7054 - accuracy: 0.6731 - val_loss: 0.8206 - val_accuracy: 0.6280\n",
      "Epoch 131/150\n",
      "4203/4203 [==============================] - 3s 758us/sample - loss: 0.7019 - accuracy: 0.6776 - val_loss: 0.8205 - val_accuracy: 0.6280\n",
      "Epoch 132/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7033 - accuracy: 0.6729 - val_loss: 0.8204 - val_accuracy: 0.6299\n",
      "Epoch 133/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7074 - accuracy: 0.6690 - val_loss: 0.8211 - val_accuracy: 0.6280\n",
      "Epoch 134/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.7015 - accuracy: 0.6743 - val_loss: 0.8212 - val_accuracy: 0.6251\n",
      "Epoch 135/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7021 - accuracy: 0.6690 - val_loss: 0.8217 - val_accuracy: 0.6232\n",
      "Epoch 136/150\n",
      "4203/4203 [==============================] - 3s 750us/sample - loss: 0.7064 - accuracy: 0.6636 - val_loss: 0.8214 - val_accuracy: 0.6261\n",
      "Epoch 137/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.7033 - accuracy: 0.6759 - val_loss: 0.8214 - val_accuracy: 0.6289\n",
      "Epoch 138/150\n",
      "4203/4203 [==============================] - 3s 737us/sample - loss: 0.7034 - accuracy: 0.6778 - val_loss: 0.8211 - val_accuracy: 0.6280\n",
      "Epoch 139/150\n",
      "4203/4203 [==============================] - 3s 744us/sample - loss: 0.7063 - accuracy: 0.6693 - val_loss: 0.8211 - val_accuracy: 0.6289\n",
      "Epoch 140/150\n",
      "4203/4203 [==============================] - 4s 923us/sample - loss: 0.7025 - accuracy: 0.6755 - val_loss: 0.8211 - val_accuracy: 0.6289\n",
      "Epoch 141/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.6959 - accuracy: 0.6745 - val_loss: 0.8215 - val_accuracy: 0.6280\n",
      "Epoch 142/150\n",
      "4203/4203 [==============================] - 3s 731us/sample - loss: 0.7060 - accuracy: 0.6667 - val_loss: 0.8218 - val_accuracy: 0.6270\n",
      "Epoch 143/150\n",
      "4203/4203 [==============================] - 3s 741us/sample - loss: 0.6998 - accuracy: 0.6769 - val_loss: 0.8214 - val_accuracy: 0.6289\n",
      "Epoch 144/150\n",
      "4203/4203 [==============================] - 3s 749us/sample - loss: 0.7010 - accuracy: 0.6712 - val_loss: 0.8219 - val_accuracy: 0.6251\n",
      "Epoch 145/150\n",
      "4203/4203 [==============================] - 3s 745us/sample - loss: 0.6980 - accuracy: 0.6757 - val_loss: 0.8216 - val_accuracy: 0.6280\n",
      "Epoch 146/150\n",
      "4203/4203 [==============================] - 3s 739us/sample - loss: 0.6960 - accuracy: 0.6817 - val_loss: 0.8220 - val_accuracy: 0.6270\n",
      "Epoch 147/150\n",
      "4203/4203 [==============================] - 3s 742us/sample - loss: 0.6990 - accuracy: 0.6790 - val_loss: 0.8223 - val_accuracy: 0.6251\n",
      "Epoch 148/150\n",
      "4203/4203 [==============================] - 3s 746us/sample - loss: 0.6980 - accuracy: 0.6805 - val_loss: 0.8227 - val_accuracy: 0.6242\n",
      "Epoch 149/150\n",
      "4203/4203 [==============================] - 3s 748us/sample - loss: 0.6991 - accuracy: 0.6857 - val_loss: 0.8232 - val_accuracy: 0.6223\n",
      "Epoch 150/150\n",
      "4203/4203 [==============================] - 3s 743us/sample - loss: 0.7024 - accuracy: 0.6833 - val_loss: 0.8227 - val_accuracy: 0.6280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f109d2647f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "EPOCHS=150\n",
    "callbacks = [TensorBoard(log_dir=f'./log/{i}')]\n",
    "sgd = optimizers.SGD(lr=0.005, decay=1e-3, momentum=0.9, nesterov=True)\n",
    "#adam = optimizers.Adam(learning_rate=0.0001, decay=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "adam = optimizers.Adam(learning_rate=0.0001, decay=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#classifier_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#classifier_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_model.fit(x=x_train, y=y_train, shuffle=True, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test))\n",
    "#classifier_model.fit(x=x_train, y=y_train, epochs=EPOCHS, callbacks=callbacks, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
